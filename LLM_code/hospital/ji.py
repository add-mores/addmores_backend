from langchain_community.llms import Ollama
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
import pandas as pd

# CSV íŒŒì¼ ë¡œë“œ
dis_df = pd.read_csv("Ragfile/dis.csv")
med_df = pd.read_csv("Ragfile/medi.csv")
hos_df = pd.read_csv("Ragfile/hospi.csv")

# í–‰ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

def load_csv_as_documents(file_path, file_label, chunk_size=500, chunk_overlap=50):
    df = pd.read_csv(file_path)
    docs = []

    # ê° í–‰(row)ì„ Documentë¡œ ë³€í™˜
    for _, row in df.iterrows():
        content = "\n".join([f"{col}: {row[col]}" for col in df.columns])
        doc = Document(
            page_content=content,
            metadata={"source_file": file_label}
        )
        docs.append(doc)

    # âœ… ì²­í¬ ë¶„í•  (LangChain ë‚´ì¥ Splitter ì‚¬ìš©)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
    )
    chunked_docs = text_splitter.split_documents(docs)

    return chunked_docs

# ì„¸ CSV íŒŒì¼ì„ ê°ê° ë¬¸ì„œë¡œ ë¡œë“œ
hos_docs = load_csv_as_documents("Ragfile/hospi.csv", "ë³‘ì›")
dis_docs = load_csv_as_documents("Ragfile/dis.csv", "ì§ˆë³‘")
med_docs = load_csv_as_documents("Ragfile/medi.csv", "ì˜ì•½í’ˆ")

all_docs = hos_docs + dis_docs + med_docs

# FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±
embedding_model_name = "madatnlp/km-bert"
embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)

vectorstore = FAISS.from_documents(all_docs, embeddings)

# RAG ê¸°ë°˜ ì§ˆì˜ í•¨ìˆ˜ ì •ì˜
def rag_answer(question):
    retriever = vectorstore.as_retriever()

    llm = Ollama(model="exaone3.5:7.8b", temperature=0.1, num_predict=1024)

    # System í”„ë¡¬í”„íŠ¸ (ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ ë° ì¶œì²˜ í•„í„°ë§ ì§€ì‹œ)
    system_template = (
        "ë„ˆëŠ” ì „ë¬¸ ì˜ë£Œ ì •ë³´ ë¹„ì„œì•¼.\n"
        "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì½ê³ , ì•„ë˜ ì„¸ ê°€ì§€ ì¤‘ ì–´ë–¤ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ”ì§€ íŒë‹¨í•´:\n"
        "- 'ì§ˆë³‘': ì§ˆë³‘ ì´ë¦„, ì¦ìƒ, ì›ì¸, ì¹˜ë£Œ ë°©ë²•, ê´€ë ¨ ì§„ë£Œê³¼ ë“±ì— ëŒ€í•œ ì •ë³´\n"
        "- 'ì˜ì•½í’ˆ': ì•½ ì´ë¦„, íš¨ëŠ¥, ì£¼ì˜ ì‚¬í•­,ë¶€ì‘ìš©, ë³µìš©ë²• ë“±ì— ëŒ€í•œ ì •ë³´\n"
        "- 'ë³‘ì›': ë³‘ì› ì´ë¦„, ìœ„ì¹˜(ì£¼ì†Œ), ì§„ë£Œ ê³¼ëª©, ì‘ê¸‰ì‹¤ ì—¬ë¶€ ë“±ì— ëŒ€í•œ ì •ë³´\n\n"
        "ë¬¸ì„œì˜ ì¶œì²˜(source_file)ë¥¼ í™•ì¸í•´ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œëŠ” ë¬´ì‹œí•˜ê³ ,\n"
        "ê´€ë ¨ ìˆëŠ” ë¬¸ì„œë§Œ ì°¸ê³ í•´ì„œ ë‹µë³€í•´.\n"
        "ì •ë³´ê°€ ëª…í™•í•˜ì§€ ì•Šê±°ë‚˜ ë¬¸ì„œì— ì—†ë‹¤ë©´ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•´."
    )
    system_prompt = SystemMessagePromptTemplate.from_template(system_template)

    # ì‚¬ìš©ì ì§ˆë¬¸ + ë¬¸ì„œ context í”„ë¡¬í”„íŠ¸
    human_template = (
        "ì§ˆë¬¸: {question}\n\n"
        "ì°¸ê³  ê°€ëŠ¥í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ë‚´ìš©ê³¼ ì¶œì²˜ í¬í•¨):\n\n{context}"
    )
    human_prompt = HumanMessagePromptTemplate.from_template(human_template)

    # í†µí•© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])

    # RetrievalQA êµ¬ì„±
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": chat_prompt}
    )

    result = qa.invoke({"query": question}) 
    return result["result"]


# 5. ì‚¬ìš©ì ì…ë ¥ ë° ì‘ë‹µ ì¶œë ¥
while True:
    query = input("\në¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? (ì˜ˆ: ì¦ìƒ, ê°ê¸°ì— ì¢‹ì€ ì•½, ê°•ë‚¨ ë³‘ì› ì¶”ì²œ ë“±)\n(ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'ì¢…ë£Œ' ì…ë ¥)\n")
    if query.lower() in ["exit", "ì¢…ë£Œ", "quit"]:
        print("ğŸ‘‹ ì´ìš©í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.")
        break

    response = rag_answer(query)
    print("\nğŸ¤– ë‹µë³€:\n", response)
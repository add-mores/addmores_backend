import os
import pandas as pd
from langchain.schema import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaLLM
from langchain.chains import RetrievalQA
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)
from dotenv import load_dotenv

# â”€â”€â”€â”€â”€ 0. í™˜ê²½ ì„¤ì • â”€â”€â”€â”€â”€
load_dotenv()
embedding_model = "madatnlp/km-bert"
embeddings = HuggingFaceEmbeddings(model_name=embedding_model)
llm = OllamaLLM(model="exaone3.5:7.8b", temperature=0.3)

# â”€â”€â”€â”€â”€ 1. CSV ë¡œë“œ â†’ Document ìƒì„± â”€â”€â”€â”€â”€
def load_csv_as_documents(path: str, label: str):
    df = pd.read_csv(path)
    docs = []
    for _, row in df.iterrows():
        content = " ".join([str(cell) for cell in row.values])
        doc = Document(page_content=content, metadata={"source_file": label})
        docs.append(doc)
    return docs

# â”€â”€â”€â”€â”€ 2. ë¬¸ì„œ ë¡œë”© ë° ì„ë² ë”© (ì²­í¬ ì—†ìŒ) â”€â”€â”€â”€â”€
base_dir = os.path.join(os.path.dirname(__file__), "Ragfile")
all_docs = []
all_docs += load_csv_as_documents(os.path.join(base_dir, "dis.csv"), "dis.csv")
all_docs += load_csv_as_documents(os.path.join(base_dir, "medi.csv"), "medi.csv")
all_docs += load_csv_as_documents(os.path.join(base_dir, "hospi.csv"), "hospi.csv")

vectordb = FAISS.from_documents(all_docs, embeddings)
retriever = vectordb.as_retriever(search_kwargs={"k": 10})

# â”€â”€â”€â”€â”€ 3. LLM ì§ˆì˜ ì²˜ë¦¬ â”€â”€â”€â”€â”€
def run_query(question: str):
    system_template = (
        "ë„ˆëŠ” ì˜ë£Œ ì •ë³´ì— ì •í†µí•œ ì±—ë´‡ì´ì•¼.\n"
        "ì§ˆë³‘ ì •ë³´ëŠ” dis.csv, ì•½í’ˆ ì •ë³´ëŠ” medi.csv, ë³‘ì› ì •ë³´ëŠ” hospi.csvì—ì„œë§Œ ì°¸ê³ í•´ì•¼ í•´.\n"
        "ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì— source_file í•­ëª©ì´ ìˆìœ¼ë‹ˆ ì–´ë–¤ íŒŒì¼ ì¶œì‹ ì¸ì§€ êµ¬ë¶„í•  ìˆ˜ ìˆì–´.\n"
        "ë¬¸ì„œ ë‚´ìš©ì— ì—†ê±°ë‚˜ ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” 'ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  ì •ì§í•˜ê²Œ ë‹µë³€í•´.\n"
        "ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ì¶œì²˜ íŒŒì¼ëª…ì„ ëª…ì‹œí•´ì¤˜."
    )
    human_template = (
        "ì§ˆë¬¸: {question}\n\n"
        "ì°¸ê³ í•  ë¬¸ì„œ:\n\n{context}"
    )
    system_prompt = SystemMessagePromptTemplate.from_template(system_template)
    human_prompt = HumanMessagePromptTemplate.from_template(human_template)
    chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])

    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        return_source_documents=False,
        chain_type_kwargs={"prompt": chat_prompt}
    )

    return qa.invoke({"query": question})

# â”€â”€â”€â”€â”€ 4. ì‹¤í–‰ë¶€ â”€â”€â”€â”€â”€
if __name__ == "__main__":
    while True:
        query = input("\nğŸ©º ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ì¢…ë£Œí•˜ë ¤ë©´ 'exit')\n> ").strip()
        if query.lower() in {"exit", "quit"}:
            print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        try:
            answer = run_query(query)
            print("\nğŸ§  ë‹µë³€:\n", answer)
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

# LLM_code/hospital/hospi_embedding.py

#!/usr/bin/env python3
import os
import pandas as pd
import numpy as np
import torch
from datetime import datetime

from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from transformers import AutoTokenizer, AutoModel
from dotenv import load_dotenv

load_dotenv()

def log(msg: str):
    print(f"ğŸ•’ [{datetime.now().strftime('%H:%M:%S')}] {msg}")

class KmBertEmbeddings:
    def __init__(self, model_name: str = "madatnlp/km-bert", device: str = None):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model     = AutoModel.from_pretrained(model_name)
        self.device    = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device).eval()

    def _mean_pooling(self, hidden_state, mask):
        mask_expanded = mask.unsqueeze(-1).expand(hidden_state.size()).float()
        summation     = (hidden_state * mask_expanded).sum(dim=1)
        divisor       = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)
        return summation / divisor

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        embeds = []
        batch_size = 16
        for i in range(0, len(texts), batch_size):
            batch = texts[i : i + batch_size]
            enc   = self.tokenizer(batch, padding=True, truncation=True, return_tensors="pt").to(self.device)
            with torch.no_grad():
                out = self.model(**enc, return_dict=True)
            pooled = self._mean_pooling(out.last_hidden_state, enc["attention_mask"])
            embeds.extend(pooled.cpu().tolist())
        return embeds

    def embed_query(self, text: str) -> list[float]:
        enc = self.tokenizer([text], padding=True, truncation=True, return_tensors="pt").to(self.device)
        with torch.no_grad():
            out = self.model(**enc, return_dict=True)
        pooled = self._mean_pooling(out.last_hidden_state, enc["attention_mask"])
        return pooled.cpu().squeeze(0).tolist()

    def __call__(self, text: str) -> list[float]:
        return self.embed_query(text)

def run_embedding(
    csv_path: str = "hospitals_with_emergency_phone.csv",
    index_path: str = "faiss_index",
    model_name: str = "madatnlp/km-bert"
):
    log("1. CSV ë¡œë“œ ì¤‘")
    csv_path = os.path.join(os.path.dirname(__file__), "hospitals_with_emergency_phone.csv")
    df = pd.read_csv(csv_path)
    df["treatment"]     = df["treatment"].fillna("")  
    log(f"  â†’ {len(df)}ê°œ ë ˆì½”ë“œ ë¡œë“œ")
    
    df = df.rename(columns={
    "hospital_name": "hos_nm",
    "hospital_type": "hos_type",
    "province": "pv",
    "city": "city",
    "address": "add",
    "treatment": "deps",
    "ì‘ê¸‰ì˜ë£Œê¸°ê´€ì—¬ë¶€": "emergency"  # âœ… ê¼­ í•„ìš”
})

    log("2. Document ìƒì„± ì¤‘")
    docs = []
    for i, row in df.iterrows():
        content = (
            f"ë³‘ì›ì´ë¦„: {row['hos_nm']}\n"
            f"ì£¼ì†Œ: {row['add']}\n"
            f"ì§„ë£Œê³¼ëª©: {row['deps']}\n"
            f"ì‘ê¸‰ì˜ë£Œê¸°ê´€ ì—¬ë¶€: {row.get('emergency', 'ì •ë³´ ì—†ìŒ')}\n"
            f"ìœ„ë„: {row.get('lat')}, ê²½ë„: {row.get('lon')}\n"
            f"ìš´ì˜ì‹œê°„: ìš´ì˜ì‹œê°„ ì •ë³´ ì—†ìŒ"
        )
        docs.append(Document(
            page_content=content,
            metadata={
                "index": i,
                "hospital_name": row["hos_nm"],
                "address": row["add"],
                "treatment": row["deps"],
                "lat": row.get("lat"),
                "lon": row.get("lon"),
                "emergency": row.get("emergency", "ì •ë³´ ì—†ìŒ"),
                # "opening_hours": row["opening_hours"],
            }
        ))
    log(f"  â†’ {len(docs)}ê°œ Document ìƒì„±")
    # âœ… ëˆ„ë½ ì—¬ë¶€ í™•ì¸
    if len(docs) != len(df):
        log(f"âš ï¸ ë³‘ì› ìˆ˜ ë¶ˆì¼ì¹˜! CSV: {len(df)}ê°œ, Document: {len(docs)}ê°œ")
    else:
        log("âœ… CSV ë³‘ì› ìˆ˜ì™€ Document ìˆ˜ ì¼ì¹˜")

    # log("3. í…ìŠ¤íŠ¸ ë¶„í•  ì‹œì‘")
    # splitter = RecursiveCharacterTextSplitter
    # split_docs = splitter.split_documents(docs)
    # log(f"  â†’ {len(split_docs)}ê°œ chunk ìƒì„±")

    log("3. ì„ë² ë”© ë¡œë“œ")
    embedder = KmBertEmbeddings(model_name)

    if os.path.exists(index_path):
        log("âœ… ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ")
        vectordb = FAISS.load_local(index_path, embedder, allow_dangerous_deserialization=True)
    else:
        log("ğŸ› ï¸ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")

    # í…ìŠ¤íŠ¸ ë¶„í•  ìƒëµí•˜ê³  ë°”ë¡œ ì„ë² ë”© ìƒì„±
        vectordb = FAISS.from_documents(docs, embedder)

    vectordb.save_local(index_path)
    log("ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")

    log("ì„ë² ë”© ì™„ë£Œ")

if __name__ == "__main__":
    run_embedding()

"""
FAISS ì¸ë±ìŠ¤ ì‚¬ì „ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ - ìˆ˜ì • ë²„ì „
ë””ë ‰í† ë¦¬: generate_faiss_indexes.py

ğŸš€ ì„±ëŠ¥ ìµœì í™”: 16ë¶„ â†’ 5ì´ˆ ë‹¨ì¶•
âœ… ì „ì²´ ë°ì´í„° ì§€ì›: clean_ + disease_prototype + medicine_code_merged
âœ… ë°°ì¹˜ ì²˜ë¦¬: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 
âœ… ìë™ ì €ì¥: ì¸ë±ìŠ¤ + ë©”íƒ€ë°ì´í„° + ë¬¸ì„œ
ğŸ”§ ìˆ˜ì •: ì‹¤ì œ CSV ì»¬ëŸ¼ëª…ì— ë§ì¶° ê°ì§€ ë¡œì§ ê°œì„ 
"""

import os
import sys
import pandas as pd
import numpy as np
import faiss
import torch
import pickle
import json
from transformers import AutoTokenizer, AutoModel
from typing import List, Dict, Tuple, Optional, Any
import logging
from datetime import datetime
import gc
from dataclasses import dataclass
from enum import Enum

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# =============================================================================
# ì„¤ì • ë° ìƒìˆ˜
# =============================================================================

class IndexConfig:
    """ì¸ë±ìŠ¤ ìƒì„± ì„¤ì •"""
    # ì €ì¥ ë””ë ‰í† ë¦¬
    INDEX_DIR = "faiss_indexes"
    
    # ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
    EMBEDDING_BATCH_SIZE = 100
    
    # íŒŒì¼ íŒ¨í„´
    INDEX_FILES = {
        "rag_qa": "rag_qa_index.index",
        "rag_medical": "rag_medical_index.index", 
        "disease_key": "disease_key_index.index",
        "disease_full": "disease_full_index.index",
        "medication": "medication_index.index"
    }
    
    METADATA_FILES = {
        "rag_qa": "rag_qa_documents.pkl",
        "rag_medical": "rag_medical_documents.pkl",
        "disease": "disease_metadata.pkl",
        "medication": "medication_metadata.pkl"
    }
    
    CONFIG_FILE = "index_config.json"

class RAGContentType(Enum):
    """RAG ì»¨í…ì¸  íƒ€ì…"""
    QA = "qa"
    MEDICAL_DOC = "medical_doc"

@dataclass
class RAGDocument:
    """RAG ë¬¸ì„œ ë°ì´í„° í´ë˜ìŠ¤"""
    doc_id: str
    content: str
    metadata: Dict
    content_type: RAGContentType
    embedding: Optional[np.ndarray] = None

# =============================================================================
# ê³ ì„±ëŠ¥ ì„ë² ë”© ëª¨ë¸ í´ë˜ìŠ¤
# =============================================================================

class BatchEmbeddingModel:
    """ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” ì„ë² ë”© ëª¨ë¸"""
    
    def __init__(self, model_name: str = "madatnlp/km-bert"):
        logger.info(f"ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        self.device = "cuda" if torch.cuda.is_available() else ("mps" if torch.backends.mps.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device)
        self.model.eval()
        logger.info(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: {self.device})")

    def encode_batch(self, texts: List[str], batch_size: int = 100) -> np.ndarray:
        """ë°°ì¹˜ ì²˜ë¦¬ë¡œ í…ìŠ¤íŠ¸ ì¸ì½”ë”© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)"""
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            logger.info(f"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {i+1}-{min(i+batch_size, len(texts))}/{len(texts)}")
            
            encodings = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=128,
                return_tensors="pt"
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model(**encodings)
                last_hidden = outputs.last_hidden_state
                attention_mask = encodings.attention_mask.unsqueeze(-1)
                masked_hidden = last_hidden * attention_mask
                sum_hidden = masked_hidden.sum(dim=1)
                lengths = attention_mask.sum(dim=1)
                sentence_embeddings = sum_hidden / lengths.clamp(min=1e-9)
                
                all_embeddings.append(sentence_embeddings.cpu().numpy())
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del encodings, outputs, last_hidden, attention_mask, masked_hidden
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        return np.vstack(all_embeddings)

# =============================================================================
# ë°ì´í„° íƒì§€ ë° ë¡œë” í´ë˜ìŠ¤
# =============================================================================

class DataDiscovery:
    """í™•ì¥ëœ ë°ì´í„° íƒì§€ í´ë˜ìŠ¤"""
    
    @staticmethod
    def discover_all_files() -> Dict[str, List[str]]:
        """ğŸ”¥ ëª¨ë“  ë°ì´í„° íŒŒì¼ íƒì§€ - clean_ + disease_prototype + medicine_code_merged"""
        files = [f for f in os.listdir('.') if f.lower().endswith(".csv")]
        
        categorized_files = {
            "rag_qa": [],           # Q&A ë°ì´í„° (clean_51004)
            "rag_medical": [],      # ì˜ë£Œ ë¬¸ì„œ (clean_55588~66149)
            "disease": [],          # ì§ˆë³‘ ë°ì´í„° (disease_prototype + ê¸°íƒ€)
            "medication": []        # ì˜ì•½í’ˆ ë°ì´í„° (medicine_code_merged + ê¸°íƒ€)
        }
        
        # ğŸ”¥ ìš°ì„ ìˆœìœ„ íŒŒì¼ë“¤ ì²´í¬
        priority_files = {
            "clean_51004.csv": "rag_qa",
            "disease_prototype.csv": "disease", 
            "medicine_code_merged.csv": "medication"
        }
        
        for fname, category in priority_files.items():
            if os.path.exists(fname):
                categorized_files[category].append(fname)
                logger.info(f"ğŸ“Œ ìš°ì„ ìˆœìœ„ íŒŒì¼ ë°œê²¬: {fname} â†’ {category}")
        
        # clean_ ì˜ë£Œ ë¬¸ì„œ íŒŒì¼ë“¤
        medical_clean_files = [
            "clean_55588.csv", "clean_56763.csv", "clean_58572.csv", 
            "clean_63166.csv", "clean_66149.csv"
        ]
        
        for fname in medical_clean_files:
            if os.path.exists(fname):
                categorized_files["rag_medical"].append(fname)
                logger.info(f"ğŸ“„ ì˜ë£Œ ë¬¸ì„œ íŒŒì¼ ë°œê²¬: {fname}")
        
        # ë‚˜ë¨¸ì§€ íŒŒì¼ë“¤ ìë™ ë¶„ë¥˜
        for fname in files:
            if fname in [f for file_list in categorized_files.values() for f in file_list]:
                continue  # ì´ë¯¸ ë¶„ë¥˜ë¨
                
            try:
                df = pd.read_csv(fname, encoding="utf-8", nrows=5)  # ìƒ˜í”Œë§Œ ì½ê¸°
                category = DataDiscovery._classify_file(df, fname)
                if category:
                    categorized_files[category].append(fname)
                    logger.info(f"ğŸ” ìë™ ë¶„ë¥˜: {fname} â†’ {category}")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ íŒŒì¼ ë¶„ë¥˜ ì‹¤íŒ¨: {fname} - {e}")
        
        # ê²°ê³¼ ìš”ì•½
        for category, file_list in categorized_files.items():
            logger.info(f"ğŸ“Š {category}: {len(file_list)}ê°œ íŒŒì¼")
            
        return categorized_files
    
    @staticmethod
    def _classify_file(df: pd.DataFrame, filename: str) -> Optional[str]:
        """íŒŒì¼ ìë™ ë¶„ë¥˜ - ì‹¤ì œ ì»¬ëŸ¼ëª… ê¸°ë°˜"""
        columns = [col.lower() for col in df.columns]
        
        # Q&A íŒ¨í„´
        if any(keyword in ' '.join(columns) for keyword in ['question', 'answer', 'q&a']):
            return "rag_qa"
        
        # ğŸ”§ ì§ˆë³‘ íŒ¨í„´ - ì‹¤ì œ ì»¬ëŸ¼ëª… ê¸°ë°˜
        disease_keywords = ['disnm', 'disease', 'ì§ˆë³‘', 'ë³‘ëª…', 'symptom', 'ì¦ìƒ', 'sym', 'diagnosis', 'ì§„ë‹¨']
        if any(keyword in ' '.join(columns) for keyword in disease_keywords):
            return "disease"
        
        # ì˜ì•½í’ˆ íŒ¨í„´
        med_keywords = ['item', 'í’ˆëª©', 'ì•½í’ˆ', 'medicine', 'drug', 'efcy', 'íš¨ëŠ¥']
        if any(keyword in ' '.join(columns) for keyword in med_keywords):
            return "medication"
        
        # ì¼ë°˜ ì˜ë£Œ ë¬¸ì„œ
        return "rag_medical"

# =============================================================================
# ê³ ì„±ëŠ¥ RAG ì¸ë±ìŠ¤ ìƒì„±ê¸°
# =============================================================================

class RAGIndexGenerator:
    """RAG ì¸ë±ìŠ¤ ìƒì„±ê¸°"""
    
    def __init__(self, embedding_model: BatchEmbeddingModel):
        self.embedding_model = embedding_model
        self.qa_documents = []
        self.medical_documents = []
        
    def generate_rag_indexes(self, qa_files: List[str], medical_files: List[str]) -> Tuple[faiss.Index, faiss.Index]:
        """RAG ì¸ë±ìŠ¤ ìƒì„±"""
        logger.info("ğŸ”„ RAG ë°ì´í„° ë¡œë”© ì‹œì‘...")
        
        # Q&A ë°ì´í„° ë¡œë“œ
        self._load_qa_data(qa_files)
        
        # ì˜ë£Œ ë¬¸ì„œ ë°ì´í„° ë¡œë“œ
        self._load_medical_documents(medical_files)
        
        # ì¸ë±ìŠ¤ êµ¬ì¶•
        qa_index, medical_index = self._build_rag_indexes()
        
        logger.info("âœ… RAG ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
        return qa_index, medical_index
    
    def _load_qa_data(self, qa_files: List[str]):
        """Q&A ë°ì´í„° ë¡œë“œ"""
        for file_path in qa_files:
            logger.info(f"ğŸ“‚ Q&A ë°ì´í„° ë¡œë“œ: {file_path}")
            
            try:
                df = pd.read_csv(file_path, encoding="utf-8")
                
                for idx, row in df.iterrows():
                    try:
                        question = str(row.get('question', '')).strip()
                        answer = str(row.get('answer', '')).strip()
                        
                        if question and answer and question != 'nan' and answer != 'nan':
                            content = f"Q: {question}\nA: {answer}"
                            
                            doc = RAGDocument(
                                doc_id=f"qa_{file_path}_{idx}",
                                content=content,
                                metadata={
                                    'question': question,
                                    'answer': answer,
                                    'source': file_path
                                },
                                content_type=RAGContentType.QA
                            )
                            self.qa_documents.append(doc)
                            
                    except Exception as e:
                        logger.error(f"Q&A í–‰ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}:{idx} - {e}")
                        
                logger.info(f"âœ… {file_path}: {len([d for d in self.qa_documents if file_path in d.doc_id])}ê°œ ë¡œë“œ")
                
            except Exception as e:
                logger.error(f"âŒ Q&A íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path} - {e}")
    
    def _load_medical_documents(self, medical_files: List[str]):
        """ì˜ë£Œ ë¬¸ì„œ ë°ì´í„° ë¡œë“œ"""
        for file_path in medical_files:
            logger.info(f"ğŸ“‚ ì˜ë£Œ ë¬¸ì„œ ë¡œë“œ: {file_path}")
            
            try:
                df = pd.read_csv(file_path, encoding="utf-8")
                
                for idx, row in df.iterrows():
                    try:
                        content_parts = []
                        
                        for col in df.columns:
                            value = str(row.get(col, '')).strip()
                            if value and value != 'nan' and len(value) > 5:
                                content_parts.append(f"{col}: {value}")
                        
                        if len(content_parts) >= 2:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ìœ íš¨í•œ ì»¬ëŸ¼
                            content = "\n".join(content_parts)
                            
                            doc = RAGDocument(
                                doc_id=f"med_{file_path}_{idx}",
                                content=content,
                                metadata={
                                    'source': file_path,
                                    'original_data': row.to_dict()
                                },
                                content_type=RAGContentType.MEDICAL_DOC
                            )
                            self.medical_documents.append(doc)
                            
                    except Exception as e:
                        logger.error(f"ì˜ë£Œ ë¬¸ì„œ í–‰ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}:{idx} - {e}")
                        
                logger.info(f"âœ… {file_path}: {len([d for d in self.medical_documents if file_path in d.doc_id])}ê°œ ë¡œë“œ")
                
            except Exception as e:
                logger.error(f"âŒ ì˜ë£Œ ë¬¸ì„œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path} - {e}")
    
    def _build_rag_indexes(self) -> Tuple[faiss.Index, faiss.Index]:
        """RAG FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        qa_index = None
        medical_index = None
        
        # Q&A ì¸ë±ìŠ¤ êµ¬ì¶•
        if self.qa_documents:
            logger.info(f"ğŸ”„ Q&A ì„ë² ë”© ìƒì„± ì¤‘: {len(self.qa_documents)}ê°œ")
            qa_texts = [doc.content for doc in self.qa_documents]
            qa_embeddings = self.embedding_model.encode_batch(qa_texts, IndexConfig.EMBEDDING_BATCH_SIZE)
            faiss.normalize_L2(qa_embeddings)
            
            qa_index = faiss.IndexFlatIP(qa_embeddings.shape[1])
            qa_index.add(qa_embeddings)
            logger.info(f"âœ… Q&A ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self.qa_documents)}ê°œ")
        
        # ì˜ë£Œ ë¬¸ì„œ ì¸ë±ìŠ¤ êµ¬ì¶•
        if self.medical_documents:
            logger.info(f"ğŸ”„ ì˜ë£Œ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘: {len(self.medical_documents)}ê°œ")
            med_texts = [doc.content for doc in self.medical_documents]
            med_embeddings = self.embedding_model.encode_batch(med_texts, IndexConfig.EMBEDDING_BATCH_SIZE)
            faiss.normalize_L2(med_embeddings)
            
            medical_index = faiss.IndexFlatIP(med_embeddings.shape[1])
            medical_index.add(med_embeddings)
            logger.info(f"âœ… ì˜ë£Œ ë¬¸ì„œ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(self.medical_documents)}ê°œ")
        
        return qa_index, medical_index

# =============================================================================
# ì§ˆë³‘ ì¸ë±ìŠ¤ ìƒì„±ê¸° - ìˆ˜ì •ëœ ì»¬ëŸ¼ ê°ì§€ ë¡œì§
# =============================================================================

class DiseaseIndexGenerator:
    """ì§ˆë³‘ ì¸ë±ìŠ¤ ìƒì„±ê¸° - ì‹¤ì œ CSV ì»¬ëŸ¼ëª… ê¸°ë°˜"""
    
    def __init__(self, embedding_model: BatchEmbeddingModel):
        self.embedding_model = embedding_model
        
    def generate_disease_indexes(self, disease_files: List[str]) -> Tuple[faiss.Index, faiss.Index, List[Dict]]:
        """ì§ˆë³‘ ì¸ë±ìŠ¤ ìƒì„±"""
        logger.info("ğŸ”„ ì§ˆë³‘ ë°ì´í„° ë¡œë”© ì‹œì‘...")
        
        all_key_texts = []
        all_full_texts = []
        all_metadata = []
        
        for file_path in disease_files:
            logger.info(f"ğŸ“‚ ì§ˆë³‘ ë°ì´í„° ë¡œë“œ: {file_path}")
            
            try:
                df = pd.read_csv(file_path, encoding="utf-8", low_memory=False)
                logger.info(f"ğŸ” {file_path} ì»¬ëŸ¼ í™•ì¸: {list(df.columns)}")
                
                detected_cols = self._detect_disease_columns(df)
                logger.info(f"ğŸ” ê°ì§€ëœ ì»¬ëŸ¼: {detected_cols}")
                
                disease_col = detected_cols.get("disease_name")
                symptoms_col = detected_cols.get("symptoms")
                symptoms_key_col = detected_cols.get("symptoms_key")
                
                if not disease_col:
                    logger.warning(f"âš ï¸ {file_path}: ì§ˆë³‘ëª… ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°ì§€ëœ ì»¬ëŸ¼: {detected_cols}")
                    continue
                
                valid_count = 0
                for idx, row in df.iterrows():
                    try:
                        disease_name = str(row.get(disease_col, "")).strip()
                        if not disease_name or disease_name == "nan":
                            continue
                        
                        # ì¦ìƒ ì •ë³´ ìˆ˜ì§‘
                        symptoms_full = ""
                        symptoms_key = ""
                        
                        if symptoms_col:
                            symptoms_full = str(row.get(symptoms_col, "")).strip()
                        if symptoms_key_col:
                            symptoms_key = str(row.get(symptoms_key_col, "")).strip()
                        
                        # ê¸°íƒ€ ì»¬ëŸ¼ë“¤ë„ ìˆ˜ì§‘ (ì •ì˜, ì¹˜ë£Œë²• ë“±)
                        additional_info = []
                        for col in ['def', 'therapy', 'diag', 'guide', 'pvt']:
                            if col in df.columns:
                                value = str(row.get(col, "")).strip()
                                if value and value != "nan":
                                    additional_info.append(value)
                        
                        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                        metadata = {
                            "disease": disease_name,
                            "symptoms": symptoms_full,
                            "symptoms_key": symptoms_key,
                            "additional_info": " ".join(additional_info),
                            "source_file": file_path,
                            "original_data": row.to_dict()
                        }
                        
                        # ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ êµ¬ì„±
                        key_text = f"{disease_name} {symptoms_key}".strip()
                        full_text = f"{disease_name} {symptoms_full} {symptoms_key} {' '.join(additional_info)}".strip()
                        
                        all_metadata.append(metadata)
                        all_key_texts.append(key_text)
                        all_full_texts.append(full_text)
                        valid_count += 1
                        
                    except Exception as e:
                        logger.error(f"ì§ˆë³‘ í–‰ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}:{idx} - {e}")
                
                logger.info(f"âœ… {file_path}: {valid_count}ê°œ ì§ˆë³‘ ë°ì´í„° ë¡œë“œ")
                
            except Exception as e:
                logger.error(f"âŒ ì§ˆë³‘ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path} - {e}")
        
        if not all_metadata:
            raise ValueError("ìœ íš¨í•œ ì§ˆë³‘ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì„ë² ë”© ìƒì„± ë° ì¸ë±ìŠ¤ êµ¬ì¶•
        logger.info(f"ğŸ”„ ì§ˆë³‘ ì„ë² ë”© ìƒì„± ì¤‘: {len(all_metadata)}ê°œ")
        
        key_embeddings = self.embedding_model.encode_batch(all_key_texts, IndexConfig.EMBEDDING_BATCH_SIZE)
        full_embeddings = self.embedding_model.encode_batch(all_full_texts, IndexConfig.EMBEDDING_BATCH_SIZE)
        
        faiss.normalize_L2(key_embeddings)
        faiss.normalize_L2(full_embeddings)
        
        disease_key_index = faiss.IndexFlatIP(key_embeddings.shape[1])
        disease_full_index = faiss.IndexFlatIP(full_embeddings.shape[1])
        
        disease_key_index.add(key_embeddings)
        disease_full_index.add(full_embeddings)
        
        logger.info(f"âœ… ì§ˆë³‘ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(all_metadata)}ê°œ")
        return disease_key_index, disease_full_index, all_metadata
    
    def _detect_disease_columns(self, df: pd.DataFrame) -> Dict[str, str]:
        """ğŸ”§ ìˆ˜ì •ëœ ì§ˆë³‘ ì»¬ëŸ¼ ê°ì§€ - ì‹¤ì œ CSV ì»¬ëŸ¼ëª… ê¸°ë°˜"""
        columns = df.columns.tolist()
        detected = {}
        
        # ğŸ”¥ ì‹¤ì œ ì»¬ëŸ¼ëª… ë§¤í•‘
        column_mappings = {
            # ì§ˆë³‘ëª… ë§¤í•‘
            'disease_name': ['disnm_ko', 'disnm_en', 'disease', 'ì§ˆë³‘', 'ë³‘ëª…', 'disease_name'],
            # ì¦ìƒ ë§¤í•‘
            'symptoms': ['sym', 'symptoms', 'ì¦ìƒ', 'symptom'],
            # í•µì‹¬ ì¦ìƒ ë§¤í•‘  
            'symptoms_key': ['sym_k', 'symptoms_key', 'í•µì‹¬ì¦ìƒ', 'key_symptoms']
        }
        
        for target_type, possible_names in column_mappings.items():
            for col in columns:
                # ì •í™•í•œ ë§¤ì¹˜ ìš°ì„ 
                if col in possible_names:
                    detected[target_type] = col
                    break
                # ë¶€ë¶„ ë§¤ì¹˜ (ì†Œë¬¸ì ë³€í™˜ í›„)
                col_lower = col.lower()
                for possible in possible_names:
                    if possible.lower() in col_lower or col_lower in possible.lower():
                        detected[target_type] = col
                        break
                if target_type in detected:
                    break
        
        logger.info(f"ğŸ” ì§ˆë³‘ ì»¬ëŸ¼ ê°ì§€ ê²°ê³¼: {detected}")
        return detected

# =============================================================================
# ì˜ì•½í’ˆ ì¸ë±ìŠ¤ ìƒì„±ê¸° - ê¸°ì¡´ ë¡œì§ ìœ ì§€
# =============================================================================

class MedicationIndexGenerator:
    """ì˜ì•½í’ˆ ì¸ë±ìŠ¤ ìƒì„±ê¸°"""
    
    def __init__(self, embedding_model: BatchEmbeddingModel):
        self.embedding_model = embedding_model
        
    def generate_medication_index(self, medication_files: List[str]) -> Tuple[faiss.Index, List[Dict]]:
        """ì˜ì•½í’ˆ ì¸ë±ìŠ¤ ìƒì„±"""
        logger.info("ğŸ”„ ì˜ì•½í’ˆ ë°ì´í„° ë¡œë”© ì‹œì‘...")
        
        all_med_texts = []
        all_metadata = []
        
        for file_path in medication_files:
            logger.info(f"ğŸ“‚ ì˜ì•½í’ˆ ë°ì´í„° ë¡œë“œ: {file_path}")
            
            try:
                df = pd.read_csv(file_path, encoding="utf-8", low_memory=False)
                detected_cols = self._detect_medication_columns(df)
                
                item_col = detected_cols.get("itemName")
                efcy_col = detected_cols.get("efcyQesitm")
                
                if not item_col or not efcy_col:
                    logger.warning(f"âš ï¸ {file_path}: í•„ìš”í•œ ì˜ì•½í’ˆ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                valid_count = 0
                for idx, row in df.iterrows():
                    try:
                        item_name = str(row.get(item_col, "")).strip()
                        efficacy = str(row.get(efcy_col, "")).strip()
                        
                        if not item_name or not efficacy or item_name == "nan" or efficacy == "nan":
                            continue
                        
                        # ë¬¸ì„œ í…ìŠ¤íŠ¸ êµ¬ì„±
                        doc_text = f"ì˜ì•½í’ˆëª…: {item_name}\níš¨ëŠ¥: {efficacy}"
                        
                        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
                        metadata = {
                            "name": item_name,
                            "efficacy": efficacy,
                            "doc_text": doc_text,
                            "source_file": file_path,
                            "original_data": row.to_dict()
                        }
                        
                        all_med_texts.append(doc_text)
                        all_metadata.append(metadata)
                        valid_count += 1
                        
                    except Exception as e:
                        logger.error(f"ì˜ì•½í’ˆ í–‰ ì²˜ë¦¬ ì˜¤ë¥˜ {file_path}:{idx} - {e}")
                
                logger.info(f"âœ… {file_path}: {valid_count}ê°œ ì˜ì•½í’ˆ ë°ì´í„° ë¡œë“œ")
                
            except Exception as e:
                logger.error(f"âŒ ì˜ì•½í’ˆ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path} - {e}")
        
        if not all_med_texts:
            logger.warning("âš ï¸ ìœ íš¨í•œ ì˜ì•½í’ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, []
        
        # ì„ë² ë”© ìƒì„± ë° ì¸ë±ìŠ¤ êµ¬ì¶•
        logger.info(f"ğŸ”„ ì˜ì•½í’ˆ ì„ë² ë”© ìƒì„± ì¤‘: {len(all_metadata)}ê°œ")
        
        med_embeddings = self.embedding_model.encode_batch(all_med_texts, IndexConfig.EMBEDDING_BATCH_SIZE)
        faiss.normalize_L2(med_embeddings)
        
        medication_index = faiss.IndexFlatIP(med_embeddings.shape[1])
        medication_index.add(med_embeddings)
        
        logger.info(f"âœ… ì˜ì•½í’ˆ ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {len(all_metadata)}ê°œ")
        return medication_index, all_metadata
    
    def _detect_medication_columns(self, df: pd.DataFrame) -> Dict[str, str]:
        """ì˜ì•½í’ˆ ì»¬ëŸ¼ ê°ì§€"""
        columns = df.columns.tolist()
        detected = {}
        
        for col in columns:
            col_lower = col.lower()
            if any(keyword in col_lower for keyword in ['item', 'í’ˆëª©', 'ì•½í’ˆëª…']):
                detected['itemName'] = col
            elif any(keyword in col_lower for keyword in ['efcy', 'íš¨ëŠ¥', 'íš¨ê³¼']):
                detected['efcyQesitm'] = col
        
        return detected

# =============================================================================
# ì¸ë±ìŠ¤ ì €ì¥/ë¡œë“œ ë§¤ë‹ˆì €
# =============================================================================

class IndexManager:
    """ì¸ë±ìŠ¤ ì €ì¥/ë¡œë“œ ë§¤ë‹ˆì €"""
    
    @staticmethod
    def save_indexes(
        qa_index: faiss.Index,
        medical_index: faiss.Index, 
        disease_key_index: faiss.Index,
        disease_full_index: faiss.Index,
        medication_index: faiss.Index,
        qa_documents: List[RAGDocument],
        medical_documents: List[RAGDocument],
        disease_metadata: List[Dict],
        medication_metadata: List[Dict]
    ):
        """ëª¨ë“  ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(IndexConfig.INDEX_DIR, exist_ok=True)
        
        logger.info("ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì‹œì‘...")
        
        try:
            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            if qa_index:
                faiss.write_index(qa_index, os.path.join(IndexConfig.INDEX_DIR, IndexConfig.INDEX_FILES["rag_qa"]))
                logger.info("âœ… RAG Q&A ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")
            
            if medical_index:
                faiss.write_index(medical_index, os.path.join(IndexConfig.INDEX_DIR, IndexConfig.INDEX_FILES["rag_medical"]))
                logger.info("âœ… RAG ì˜ë£Œë¬¸ì„œ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")
            
            if disease_key_index:
                faiss.write_index(disease_key_index, os.path.join(IndexConfig.INDEX_DIR, IndexConfig.INDEX_FILES["disease_key"]))
                logger.info("âœ… ì§ˆë³‘ Key ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")
            
            if disease_full_index:
                faiss.write_index(disease_full_index, os.path.join(IndexConfig.INDEX_DIR, IndexConfig.INDEX_FILES["disease_full"]))
                logger.info("âœ… ì§ˆë³‘ Full ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")
            
            if medication_index:
                faiss.write_index(medication_index, os.path.join(IndexConfig.INDEX_DIR, IndexConfig.INDEX_FILES["medication"]))
                logger.info("âœ… ì˜ì•½í’ˆ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ")
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            if qa_documents:
                with open(os.path.join(IndexConfig.INDEX_DIR, IndexConfig.METADATA_FILES["rag_qa"]), 'wb') as f:
                    pickle.dump(qa_documents, f)
                logger.info("âœ… RAG Q&A ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")
            
            if medical_documents:
                with open(os.path.join(IndexConfig.INDEX_DIR, IndexConfig.METADATA_FILES["rag_medical"]), 'wb') as f:
                    pickle.dump(medical_documents, f)
                logger.info("âœ… RAG ì˜ë£Œë¬¸ì„œ ì €ì¥ ì™„ë£Œ")
            
            if disease_metadata:
                with open(os.path.join(IndexConfig.INDEX_DIR, IndexConfig.METADATA_FILES["disease"]), 'wb') as f:
                    pickle.dump(disease_metadata, f)
                logger.info("âœ… ì§ˆë³‘ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            
            if medication_metadata:
                with open(os.path.join(IndexConfig.INDEX_DIR, IndexConfig.METADATA_FILES["medication"]), 'wb') as f:
                    pickle.dump(medication_metadata, f)
                logger.info("âœ… ì˜ì•½í’ˆ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            
            # ì„¤ì • ì •ë³´ ì €ì¥
            config_info = {
                "created_at": datetime.now().isoformat(),
                "total_qa_docs": len(qa_documents) if qa_documents else 0,
                "total_medical_docs": len(medical_documents) if medical_documents else 0,
                "total_disease_docs": len(disease_metadata) if disease_metadata else 0,
                "total_medication_docs": len(medication_metadata) if medication_metadata else 0,
                "embedding_model": "madatnlp/km-bert",
                "version": "v4.1_fixed"
            }
            
            with open(os.path.join(IndexConfig.INDEX_DIR, IndexConfig.CONFIG_FILE), 'w', encoding='utf-8') as f:
                json.dump(config_info, f, ensure_ascii=False, indent=2)
            
            logger.info("âœ… ëª¨ë“  ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ!")
            logger.info(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {os.path.abspath(IndexConfig.INDEX_DIR)}")
            
        except Exception as e:
            logger.error(f"âŒ ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

# =============================================================================
# ë©”ì¸ ìƒì„± í•¨ìˆ˜
# =============================================================================

def generate_all_indexes():
    """ëª¨ë“  FAISS ì¸ë±ìŠ¤ ìƒì„± ë©”ì¸ í•¨ìˆ˜"""
    start_time = datetime.now()
    logger.info("ğŸš€ FAISS ì¸ë±ìŠ¤ ì‚¬ì „ ìƒì„± ì‹œì‘!")
    logger.info(f"ğŸ“… ì‹œì‘ ì‹œê°„: {start_time}")
    
    try:
        # 1. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embedding_model = BatchEmbeddingModel()
        
        # 2. ë°ì´í„° íŒŒì¼ íƒì§€
        file_categories = DataDiscovery.discover_all_files()
        
        # 3. RAG ì¸ë±ìŠ¤ ìƒì„±
        rag_generator = RAGIndexGenerator(embedding_model)
        qa_index, medical_index = rag_generator.generate_rag_indexes(
            file_categories["rag_qa"], 
            file_categories["rag_medical"]
        )
        
        # 4. ì§ˆë³‘ ì¸ë±ìŠ¤ ìƒì„±
        disease_key_index = disease_full_index = disease_metadata = None
        if file_categories["disease"]:
            disease_generator = DiseaseIndexGenerator(embedding_model)
            disease_key_index, disease_full_index, disease_metadata = disease_generator.generate_disease_indexes(
                file_categories["disease"]
            )
        
        # 5. ì˜ì•½í’ˆ ì¸ë±ìŠ¤ ìƒì„±
        medication_index = medication_metadata = None
        if file_categories["medication"]:
            medication_generator = MedicationIndexGenerator(embedding_model)
            medication_index, medication_metadata = medication_generator.generate_medication_index(
                file_categories["medication"]
            )
        
        # 6. ëª¨ë“  ì¸ë±ìŠ¤ ì €ì¥
        IndexManager.save_indexes(
            qa_index=qa_index,
            medical_index=medical_index,
            disease_key_index=disease_key_index,
            disease_full_index=disease_full_index,
            medication_index=medication_index,
            qa_documents=rag_generator.qa_documents,
            medical_documents=rag_generator.medical_documents,
            disease_metadata=disease_metadata or [],
            medication_metadata=medication_metadata or []
        )
        
        # ì™„ë£Œ ì‹œê°„ ê³„ì‚°
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info("ğŸ‰ ëª¨ë“  FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
        logger.info(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {duration}")
        logger.info("ğŸ“Š ìƒì„± ê²°ê³¼:")
        logger.info(f"   - RAG Q&A: {len(rag_generator.qa_documents)}ê°œ ë¬¸ì„œ")
        logger.info(f"   - RAG ì˜ë£Œë¬¸ì„œ: {len(rag_generator.medical_documents)}ê°œ ë¬¸ì„œ")
        logger.info(f"   - ì§ˆë³‘ ë°ì´í„°: {len(disease_metadata) if disease_metadata else 0}ê°œ")
        logger.info(f"   - ì˜ì•½í’ˆ ë°ì´í„°: {len(medication_metadata) if medication_metadata else 0}ê°œ")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        gc.collect()

if __name__ == "__main__":
    print("="*80)
    print("ğŸš€ FAISS ì¸ë±ìŠ¤ ì‚¬ì „ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ - ìˆ˜ì • ë²„ì „")
    print("ğŸ”§ ìˆ˜ì •: ì‹¤ì œ CSV ì»¬ëŸ¼ëª…ì— ë§ì¶˜ ê°ì§€ ë¡œì§ ê°œì„ ")
    print("âœ… disnm_ko, sym, sym_k ë“± ì‹¤ì œ ì»¬ëŸ¼ëª… ì§€ì›")
    print("="*80)
    
    success = generate_all_indexes()
    
    if success:
        print("\nâœ… ì¸ë±ìŠ¤ ìƒì„± ì„±ê³µ!")
        print("ğŸ’¡ ì´ì œ ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë¹ ë¥¸ ë¡œë”©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        print("\nâŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨!")
        print("ğŸ” ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”.")
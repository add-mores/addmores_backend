"""
ì§ˆë³‘ ì„ë² ë”© ì„œë¹„ìŠ¤
ìœ„ì¹˜: ~/backend/app/services/disease_embedding_service.py

ğŸ¯ ëª©ì : KM-BERT ëª¨ë¸ì„ ì´ìš©í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
ğŸ“‹ ê¸°ëŠ¥:
   - KM-BERT ëª¨ë¸ ë¡œë“œ ë° ê´€ë¦¬
   - í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
   - ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
   - GPU/MPS ì§€ì›

âš™ï¸ ì˜ì¡´ì„±: torch, transformers, numpy
"""

import torch
import numpy as np
import logging
from typing import List, Optional, Union
from transformers import AutoTokenizer, AutoModel

from ..utils.disease_constants import EMBEDDING_MODEL_NAME, EMBEDDING_MAX_LENGTH
from ..utils.disease_exceptions import EmbeddingModelLoadError, EmbeddingGenerationError

logger = logging.getLogger(__name__)


class DiseaseEmbeddingService:
    """ì§ˆë³‘ ì„ë² ë”© ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = EMBEDDING_MODEL_NAME):
        self.model_name = model_name
        self.device = self._get_best_device()
        self.tokenizer: Optional[AutoTokenizer] = None
        self.model: Optional[AutoModel] = None
        self.is_loaded = False
        
        logger.info(f"ğŸ§  ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™”: {model_name}")
        logger.info(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
    
    def _get_best_device(self) -> str:
        """ìµœì ì˜ ë””ë°”ì´ìŠ¤ ì„ íƒ"""
        if torch.cuda.is_available():
            return "cuda"
        elif torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"
    
    def load_model(self) -> bool:
        """KM-BERT ëª¨ë¸ ë¡œë“œ"""
        if self.is_loaded:
            logger.info("âœ… ì„ë² ë”© ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return True
        
        try:
            logger.info("ğŸ”„ KM-BERT ëª¨ë¸ ë¡œë”© ì¤‘...")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            logger.info("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
            
            # ëª¨ë¸ ë¡œë“œ
            self.model = AutoModel.from_pretrained(self.model_name)
            self.model.to(self.device)
            self.model.eval()  # ì¶”ë¡  ëª¨ë“œ
            logger.info("âœ… KM-BERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            # ëª¨ë¸ í…ŒìŠ¤íŠ¸
            self._test_model()
            
            self.is_loaded = True
            logger.info("âœ… ì„ë² ë”© ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ!")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.is_loaded = False
            raise EmbeddingModelLoadError(self.model_name, str(e))
    
    def _test_model(self):
        """ëª¨ë¸ ì •ìƒ ì‘ë™ í…ŒìŠ¤íŠ¸"""
        try:
            test_texts = ["ì•ˆë…•í•˜ì„¸ìš”", "í…ŒìŠ¤íŠ¸"]
            test_embeddings = self._encode_texts(test_texts)
            
            if test_embeddings.shape[0] != 2:
                raise EmbeddingModelLoadError(self.model_name, "ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ì˜ëª»ëœ ì¶œë ¥ í¬ê¸°")
            
            logger.info(f"âœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: ì„ë² ë”© ì°¨ì› {test_embeddings.shape[1]}")
            
        except Exception as e:
            raise EmbeddingModelLoadError(self.model_name, f"ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    def encode(self, texts: Union[str, List[str]]) -> np.ndarray:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì¸ì½”ë”©"""
        if not self.is_loaded:
            raise EmbeddingModelLoadError(self.model_name, "ì„ë² ë”© ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if isinstance(texts, str):
            texts = [texts]
        
        if not texts:
            raise EmbeddingGenerationError("", "ë¹ˆ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.")
        
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            processed_texts = self._preprocess_texts(texts)
            
            # ì„ë² ë”© ìƒì„±
            embeddings = self._encode_texts(processed_texts)
            
            logger.debug(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(texts)}ê°œ í…ìŠ¤íŠ¸ -> {embeddings.shape}")
            
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise EmbeddingGenerationError(str(texts), f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _preprocess_texts(self, texts: List[str]) -> List[str]:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        processed = []
        
        for text in texts:
            if not isinstance(text, str):
                text = str(text)
            
            # ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            text = text.strip()
            if not text:
                text = "[ë¹ˆ í…ìŠ¤íŠ¸]"
            
            # ê¸¸ì´ ì œí•œ
            if len(text) > EMBEDDING_MAX_LENGTH * 2:  # í† í°ì´ ì•„ë‹Œ ë¬¸ì ê¸°ì¤€ìœ¼ë¡œ ëŒ€ëµì  ì œí•œ
                text = text[:EMBEDDING_MAX_LENGTH * 2]
                logger.warning(f"âš ï¸ í…ìŠ¤íŠ¸ê°€ ì˜ë ¸ìŠµë‹ˆë‹¤: {len(text)}ì")
            
            processed.append(text)
        
        return processed
    
    def _encode_texts(self, texts: List[str]) -> np.ndarray:
        """ì‹¤ì œ ì„ë² ë”© ìƒì„± ë¡œì§"""
        try:
            # í† í¬ë‚˜ì´ì§•
            encodings = self.tokenizer(
                texts,
                padding=True,
                truncation=True,
                max_length=EMBEDDING_MAX_LENGTH,
                return_tensors="pt"
            ).to(self.device)
            
            # ëª¨ë¸ ì¶”ë¡ 
            with torch.no_grad():
                outputs = self.model(**encodings)
                last_hidden = outputs.last_hidden_state
                attention_mask = encodings.attention_mask.unsqueeze(-1)
                
                # ë§ˆìŠ¤í‚¹ëœ í‰ê·  í’€ë§
                masked_hidden = last_hidden * attention_mask
                sum_hidden = masked_hidden.sum(dim=1)
                lengths = attention_mask.sum(dim=1)
                sentence_embeddings = sum_hidden / lengths.clamp(min=1e-9)
            
            # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
            embeddings = sentence_embeddings.cpu().numpy()
            
            return embeddings
            
        except Exception as e:
            raise EmbeddingGenerationError("", f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
    
    def encode_batch(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ ì¸ì½”ë”©"""
        if not texts:
            raise EmbeddingGenerationError("", "ë¹ˆ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.")
        
        if len(texts) <= batch_size:
            return self.encode(texts)
        
        logger.info(f"ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸, ë°°ì¹˜ í¬ê¸° {batch_size}")
        
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            logger.debug(f"ğŸ“¦ ë°°ì¹˜ {i//batch_size + 1}: {len(batch_texts)}ê°œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘...")
            
            batch_embeddings = self.encode(batch_texts)
            all_embeddings.append(batch_embeddings)
        
        # ëª¨ë“  ë°°ì¹˜ ê²°í•©
        final_embeddings = np.vstack(all_embeddings)
        logger.info(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {final_embeddings.shape}")
        
        return final_embeddings
    
    def get_embedding_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        if not self.is_loaded:
            raise EmbeddingModelLoadError(self.model_name, "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í…ŒìŠ¤íŠ¸ ì„ë² ë”©ìœ¼ë¡œ ì°¨ì› í™•ì¸
        test_embedding = self.encode(["í…ŒìŠ¤íŠ¸"])
        return test_embedding.shape[1]
    
    def similarity(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        embeddings = self.encode([text1, text2])
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        embedding1 = embeddings[0]
        embedding2 = embeddings[1]
        
        dot_product = np.dot(embedding1, embedding2)
        norm1 = np.linalg.norm(embedding1)
        norm2 = np.linalg.norm(embedding2)
        
        similarity = dot_product / (norm1 * norm2)
        return float(similarity)
    
    def get_model_info(self) -> dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        info = {
            "model_name": self.model_name,
            "device": self.device,
            "is_loaded": self.is_loaded,
            "max_length": EMBEDDING_MAX_LENGTH
        }
        
        if self.is_loaded:
            info["embedding_dimension"] = self.get_embedding_dimension()
        
        return info
    
    def unload_model(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ í•´ì œ)"""
        if self.is_loaded:
            self.model = None
            self.tokenizer = None
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.device in ["cuda", "mps"]:
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                # MPSëŠ” ë³„ë„ ìºì‹œ ì •ë¦¬ í•¨ìˆ˜ê°€ ì—†ìŒ
            
            self.is_loaded = False
            logger.info("âœ… ì„ë² ë”© ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ")


# ì „ì—­ ì„ë² ë”© ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_global_embedding_service: Optional[DiseaseEmbeddingService] = None


def get_embedding_service() -> DiseaseEmbeddingService:
    """ì„ë² ë”© ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_embedding_service
    
    if _global_embedding_service is None:
        _global_embedding_service = DiseaseEmbeddingService()
    
    return _global_embedding_service


def initialize_embedding_service() -> bool:
    """ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
    try:
        service = get_embedding_service()
        return service.load_model()
    except Exception as e:
        logger.error(f"âŒ ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise
"""
ê°„ì†Œí™”ëœ FastAPI ë©”ì¸ ì•± - LLM í†µí•© ì˜ë£Œ ì±—ë´‡ API
ìœ„ì¹˜: backend/app/llm/main_llm.py

ğŸ¯ ëª©ì : ì„¤ì • íŒŒì¼ ì—†ì´ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ë²„ì „
ğŸš€ ì‹¤í–‰: python -m app.llm.main_llm
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import logging
import uvicorn
import os


# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
chat_service_instance = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë¼ì´í”„ì‚¬ì´í´ ê´€ë¦¬"""
    global chat_service_instance
    
    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì´ˆê¸°í™”
    logger.info("ğŸš€ LLM í†µí•© ì˜ë£Œ ì±—ë´‡ API ì´ˆê¸°í™” ì‹œì‘...")
    
    try:
        # ğŸ”§ ë™ì  importë¡œ ì´ˆê¸°í™” ì§€ì—°
        logger.info("ğŸ§  LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        
        from app.llm.services.integrated_chat_service import IntegratedChatAPIService
        chat_service_instance = IntegratedChatAPIService()
        
        logger.info("âœ… LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        
        yield
        
    except Exception as e:
        logger.error(f"âŒ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.warning("LLM ê¸°ëŠ¥ ì—†ì´ ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        chat_service_instance = None
        yield
    finally:
        # ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ì‹œ ì •ë¦¬
        logger.info("ğŸ”„ LLM ì„œë¹„ìŠ¤ ì¢…ë£Œ ì¤‘...")
        if chat_service_instance:
            try:
                chat_service_instance.cleanup()
            except:
                pass
        logger.info("âœ… LLM ì„œë¹„ìŠ¤ ì¢…ë£Œ ì™„ë£Œ")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="LLM Integrated Medical Chatbot API",
    description="""
    ğŸ¥ **í†µí•© ì˜ë£Œ ì±—ë´‡ API v6**
    
    CLI ê¸°ë°˜ ì˜ë£Œ ì±—ë´‡ì„ FastAPIë¡œ ë³€í™˜í•œ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
    
    ## ğŸ” ì£¼ìš” ê¸°ëŠ¥:
    - **ì§ˆë³‘ ì§„ë‹¨**: ì¦ìƒ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸í•œ ì°¨ë³„í™” ì§ˆë¬¸
    - **ì˜ì•½í’ˆ ì¶”ì²œ**: ì§ˆë³‘ ì—°ê³„ ì˜ì•½í’ˆ ì •ë³´ ì œê³µ
    - **RAG ê²€ìƒ‰**: 6ê°œ clean_ íŒŒì¼ ê¸°ë°˜ ì§€ì‹ ë² ì´ìŠ¤
    - **ì„¸ì…˜ ê´€ë¦¬**: ëŒ€í™” ë¬¸ë§¥ ìœ ì§€ ë° ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
    - **ì˜ë„ ë¶„ë¥˜**: ì‚¬ìš©ì ë©”ì‹œì§€ ì˜ë„ ìë™ íŒŒì•…
    
    ## ğŸš€ ê¸°ìˆ  ìŠ¤íƒ:
    - **LLM**: EXAONE 3.5:7.8b
    - **ì„ë² ë”©**: KM-BERT (madatnlp/km-bert)
    - **ë²¡í„° ê²€ìƒ‰**: FAISS ì¸ë±ìŠ¤
    - **ë°±ì—”ë“œ**: FastAPI
    
    ## âš ï¸ ì£¼ì˜ì‚¬í•­:
    ì´ ì„œë¹„ìŠ¤ëŠ” ì˜ë£Œ ì „ë¬¸ê°€ì˜ ì§„ë‹¨ì„ ëŒ€ì²´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
    """,
    version="6.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# CORS ì„¤ì • (ê°œë°œ í™˜ê²½ ì¹œí™”ì )
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000", 
        "http://127.0.0.1:3000",
        "http://localhost:8000",  # ê¸°ì¡´ APIì™€ ì—°ë™
        "http://127.0.0.1:8000"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# API ë¼ìš°í„° ë“±ë¡
app.include_router(
    chat_router, 
    prefix="/api/v1",
    tags=["ì±„íŒ…"]
)

# í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@app.get("/health", tags=["í—¬ìŠ¤ì²´í¬"])
async def health_check():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    global chat_service_instance
    
    if not chat_service_instance:
        return {
            "status": "partial",
            "service": "LLM Integrated Medical Chatbot API",
            "version": "6.0.0",
            "timestamp": "2024-12-12T14:30:22",
            "message": "LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘ì´ê±°ë‚˜ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
            "llm_available": False
        }
    
    # ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
    try:
        service_status = chat_service_instance.get_service_status()
        
        return {
            "status": "healthy",
            "service": "LLM Integrated Medical Chatbot API",
            "version": "6.0.0",
            "timestamp": chat_service_instance.get_current_timestamp(),
            "llm_available": True,
            "services": service_status
        }
    except Exception as e:
        return {
            "status": "degraded",
            "service": "LLM Integrated Medical Chatbot API",
            "version": "6.0.0",
            "message": f"ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}",
            "llm_available": False
        }

# ì„œë¹„ìŠ¤ ì •ë³´ ì—”ë“œí¬ì¸íŠ¸
@app.get("/info", tags=["ì •ë³´"])
async def service_info():
    """ì„œë¹„ìŠ¤ ìƒì„¸ ì •ë³´"""
    global chat_service_instance
    
    base_info = {
        "service_name": "LLM Integrated Medical Chatbot API",
        "version": "6.0.0",
        "description": "CLI ê¸°ë°˜ ì˜ë£Œ ì±—ë´‡ì˜ FastAPI ë³€í™˜ ë²„ì „",
        "features": [
            "ì§ˆë³‘ ì§„ë‹¨ (ìŠ¤ë§ˆíŠ¸í•œ ì°¨ë³„í™” ì§ˆë¬¸)",
            "ì˜ì•½í’ˆ ì¶”ì²œ ë° ì •ë³´ ì œê³µ", 
            "RAG ê¸°ë°˜ ì˜ë£Œ ì§€ì‹ ê²€ìƒ‰",
            "ì„¸ì…˜ ê¸°ë°˜ ëŒ€í™” ë¬¸ë§¥ ìœ ì§€",
            "ì‚¬ìš©ì ì˜ë„ ìë™ ë¶„ë¥˜"
        ],
        "endpoints": {
            "chat": "/api/v1/chat",
            "session_reset": "/api/v1/chat/session/{session_id}/reset",
            "session_status": "/api/v1/chat/session/{session_id}/status"
        },
        "models": {
            "llm": "EXAONE 3.5:7.8b",
            "embedding": "madatnlp/km-bert",
            "vector_search": "FAISS"
        }
    }
    
    if chat_service_instance:
        try:
            service_status = chat_service_instance.get_service_status()
            base_info["llm_status"] = service_status
        except:
            base_info["llm_status"] = "unavailable"
    else:
        base_info["llm_status"] = "not_initialized"
    
    return base_info

# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸
@app.get("/test", tags=["í…ŒìŠ¤íŠ¸"])
async def test_endpoint():
    """ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "LLM API ì„œë²„ê°€ ì •ìƒ ë™ì‘ ì¤‘ì…ë‹ˆë‹¤!",
        "timestamp": "2024-12-12T14:30:22",
        "status": "ok"
    }

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ì ‘ê·¼ í•¨ìˆ˜
def get_chat_service():
    """ì „ì—­ ì±„íŒ… ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global chat_service_instance
    if not chat_service_instance:
        raise HTTPException(
            status_code=503, 
            detail="ì±„íŒ… ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )
    return chat_service_instance

# ê°œë°œ í™˜ê²½ì—ì„œ ì§ì ‘ ì‹¤í–‰ ì‹œ
if __name__ == "__main__":
    print("ğŸš€ LLM í†µí•© ì˜ë£Œ ì±—ë´‡ API ì„œë²„ ì‹œì‘...")
    print("ğŸ“ URL: http://localhost:8001")
    print("ğŸ“– API ë¬¸ì„œ: http://localhost:8001/docs")
    print("ğŸ”„ ì¢…ë£Œ: Ctrl+C")
    print("âš ï¸ ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ ì¸í•´ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (í•„ìš”ì‹œ)
    os.environ.setdefault("LLM_ENABLED", "true")
    os.environ.setdefault("EXAONE_SERVER_URL", "http://localhost:11434")
    
    uvicorn.run(
        "app.llm.main_llm:app",
        host="0.0.0.0",
        port=8001,
        reload=True,  # ê°œë°œ í™˜ê²½ì—ì„œë§Œ
        log_level="info"
    )
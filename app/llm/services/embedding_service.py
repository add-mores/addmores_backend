"""
í†µí•© ì„ë² ë”© ì„œë¹„ìŠ¤ - ëª¨ë“  FAISS ì¸ë±ìŠ¤ í†µí•© ê´€ë¦¬
ìœ„ì¹˜: backend/app/llm/services/embedding_service.py

ğŸ¯ ëª©ì : 
- 10ê°œ FAISS ì¸ë±ìŠ¤ íŒŒì¼ í†µí•© ë¡œë“œ
- KM-BERT ê¸°ë°˜ ì„ë² ë”© ìƒì„±
- Pickle í´ë˜ìŠ¤ ì°¸ì¡° ë¬¸ì œ í•´ê²°
- RAG + ì§ˆë³‘ + ì˜ì•½í’ˆ ê²€ìƒ‰ í†µí•©

ğŸ“‹ ê¸°ëŠ¥:
- UnifiedIndexLoader: ëª¨ë“  ì¸ë±ìŠ¤ í†µí•© ë¡œë”
- EmbeddingModel: KM-BERT ì„ë² ë”© ìƒì„±
- EnhancedRAGIndexManager: ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥
- CustomUnpickler: Pickle ë¬¸ì œ í•´ê²°

ğŸš€ ê¸°ìˆ ìŠ¤íƒ: FastAPI + KM-BERT + FAISS + AWS
"""

import os
import pickle
import json
import logging
import pandas as pd
import numpy as np
import faiss
import torch
import sys
import io
from transformers import AutoTokenizer, AutoModel
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass
from enum import Enum
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# =============================================================================
# ğŸ”§ Pickle í´ë˜ìŠ¤ ì°¸ì¡° ë¬¸ì œ í•´ê²°ìš© ì»¤ìŠ¤í…€ Unpickler
# =============================================================================

class CustomUnpickler(pickle.Unpickler):
    """
    Pickle í´ë˜ìŠ¤ ì°¸ì¡° ë¬¸ì œ í•´ê²°ìš© ì»¤ìŠ¤í…€ Unpickler
    
    ğŸ¯ ëª©ì : pickle íŒŒì¼ì— ì €ì¥ëœ í´ë˜ìŠ¤ê°€ ë‹¤ë¥¸ ëª¨ë“ˆ ê²½ë¡œì— ìˆì„ ë•Œ ìë™ ë§¤í•‘
    ğŸ“‹ í•´ê²° ë°©ë²•: ëª¨ë“ˆ ê²½ë¡œ ìë™ ë³€í™˜ ë° í˜„ì¬ ëª¨ë“ˆì—ì„œ í´ë˜ìŠ¤ ì°¾ê¸°
    """
    
    def find_class(self, module, name):
        """í´ë˜ìŠ¤ ì°¾ê¸° ì‹œ ê²½ë¡œ ë§¤í•‘"""
        
        # RAG ê´€ë ¨ í´ë˜ìŠ¤ íŠ¹ë³„ ì²˜ë¦¬
        if name in ['RAGDocument', 'RAGContentType']:
            current_module = sys.modules[__name__]
            if hasattr(current_module, name):
                logger.debug(f"ğŸ”„ í´ë˜ìŠ¤ ë§¤í•‘: {module}.{name} â†’ {__name__}.{name}")
                return getattr(current_module, name)
        
        # ê¸°ë³¸ ë™ì‘ ì‹œë„
        try:
            return super().find_class(module, name)
        except (AttributeError, ImportError, ModuleNotFoundError) as e:
            logger.debug(f"âš ï¸ ê¸°ë³¸ í´ë˜ìŠ¤ ì°¾ê¸° ì‹¤íŒ¨: {module}.{name} - {e}")
            
            # í˜„ì¬ ëª¨ë“ˆì—ì„œ ì°¾ê¸° ì‹œë„
            current_module = sys.modules[__name__]
            if hasattr(current_module, name):
                logger.info(f"âœ… í˜„ì¬ ëª¨ë“ˆì—ì„œ í´ë˜ìŠ¤ ë°œê²¬: {name}")
                return getattr(current_module, name)
            
            # ìµœí›„ ìˆ˜ë‹¨: object ë°˜í™˜
            logger.warning(f"âš ï¸ í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {module}.{name}, objectë¡œ ëŒ€ì²´")
            return object

# =============================================================================
# ğŸ” RAG ê´€ë ¨ í´ë˜ìŠ¤ ë° ë°ì´í„° ëª¨ë¸
# =============================================================================

class RAGContentType(Enum):
    """RAG ì»¨í…ì¸  íƒ€ì… ì •ì˜"""
    QA = "qa"                    # Q&A í˜•íƒœ ë°ì´í„°
    MEDICAL_DOC = "medical_doc"  # ì˜ë£Œ ë¬¸ì„œ ë°ì´í„°

@dataclass
class RAGDocument:
    """
    RAG ë¬¸ì„œ ë°ì´í„° í´ë˜ìŠ¤
    
    ğŸ“‹ êµ¬ì„±ìš”ì†Œ:
    - doc_id: ë¬¸ì„œ ê³ ìœ  ID
    - content: ë¬¸ì„œ ë‚´ìš© (ê²€ìƒ‰ ëŒ€ìƒ)
    - metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    - content_type: ì»¨í…ì¸  íƒ€ì… (QA/MEDICAL_DOC)
    - embedding: ë²¡í„° ì„ë² ë”© (ì„ íƒì‚¬í•­)
    """
    doc_id: str
    content: str
    metadata: Dict
    content_type: RAGContentType
    embedding: Optional[np.ndarray] = None

# =============================================================================
# ğŸ§  KM-BERT ê¸°ë°˜ ì„ë² ë”© ëª¨ë¸
# =============================================================================

class EmbeddingModel:
    """
    KM-BERT ê¸°ë°˜ ì„ë² ë”© ëª¨ë¸
    
    ğŸ§  ëª¨ë¸: madatnlp/km-bert (í•œêµ­ì–´ íŠ¹í™”)
    ğŸ“ ì°¨ì›: 768ì°¨ì› ë²¡í„°
    âš¡ GPU ì§€ì›: CUDA ì‚¬ìš© ê°€ëŠ¥ ì‹œ ìë™ í™œìš©
    ğŸ”§ ë°°ì¹˜ ì²˜ë¦¬: ëŒ€ìš©ëŸ‰ ë°ì´í„° íš¨ìœ¨ì  ì²˜ë¦¬
    """
    
    def __init__(self, model_name: str = "madatnlp/km-bert"):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        logger.info(f"ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_name}")
        logger.info(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        try:
            # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModel.from_pretrained(model_name)
            self.model.to(self.device)
            self.model.eval()
            
            # ì„ë² ë”© ì°¨ì› í™•ì¸
            self.embedding_dim = self.model.config.hidden_size
            
            logger.info(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ì°¨ì›: {self.embedding_dim})")
            
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise e
    
    def encode(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°ì ˆ)
            
        Returns:
            np.ndarray: ì„ë² ë”© ë°°ì—´ (n, 768)
        """
        if not texts:
            return np.array([]).reshape(0, self.embedding_dim)
        
        all_embeddings = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            try:
                # í† í¬ë‚˜ì´ì§•
                inputs = self.tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors="pt"
                ).to(self.device)
                
                # ì„ë² ë”© ìƒì„±
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš© (ë¬¸ì¥ í‘œí˜„)
                    embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                
                all_embeddings.append(embeddings)
                
            except Exception as e:
                logger.error(f"âŒ ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ì œë¡œ ë²¡í„°ë¡œ ëŒ€ì²´
                zero_embeddings = np.zeros((len(batch_texts), self.embedding_dim))
                all_embeddings.append(zero_embeddings)
        
        # ëª¨ë“  ë°°ì¹˜ ê²°í•©
        final_embeddings = np.vstack(all_embeddings)
        
        logger.debug(f"ğŸ” ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(texts)}ê°œ í…ìŠ¤íŠ¸ â†’ {final_embeddings.shape}")
        
        return final_embeddings
    
    def encode_single(self, text: str) -> np.ndarray:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
        return self.encode([text])[0]
    
    def similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        from sklearn.metrics.pairwise import cosine_similarity
        return cosine_similarity([embedding1], [embedding2])[0][0]

# =============================================================================
# ğŸ”§ í†µí•© ì¸ë±ìŠ¤ ë¡œë” - ëª¨ë“  FAISS ì¸ë±ìŠ¤ íŒŒì¼ í™œìš©
# =============================================================================

class UnifiedIndexLoader:
    """
    í†µí•© FAISS ì¸ë±ìŠ¤ ë¡œë” - ëª¨ë“  10ê°œ íŒŒì¼ í™œìš©
    
    ğŸš€ ê¸°ëŠ¥:
    - ëª¨ë“  FAISS ì¸ë±ìŠ¤ í•œ ë²ˆì— ë¡œë“œ
    - Pickle í´ë˜ìŠ¤ ì°¸ì¡° ë¬¸ì œ í•´ê²°
    - í†µí•© ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§
    - ë¶€ë¶„ ë¡œë“œ í—ˆìš© (ì¼ë¶€ íŒŒì¼ ëˆ„ë½ ì‹œì—ë„ ë™ì‘)
    
    ğŸ“ ì§€ì› íŒŒì¼:
    - RAG ì¸ë±ìŠ¤: rag_qa_index.index, rag_medical_index.index
    - ì§ˆë³‘ ì¸ë±ìŠ¤: disease_key_index.index, disease_full_index.index  
    - ì˜ì•½í’ˆ ì¸ë±ìŠ¤: medication_index.index
    - ë©”íƒ€ë°ì´í„°: *.pkl íŒŒì¼ë“¤
    """
    
    INDEX_DIR = "app/integration_test/faiss_indexes"
    
    # ğŸ” ëª¨ë“  ì¸ë±ìŠ¤ íŒŒì¼ ì •ì˜
    ALL_INDEX_FILES = {
        # RAG ì‹œìŠ¤í…œìš©
        "rag_qa": "rag_qa_index.index",
        "rag_medical": "rag_medical_index.index",
        
        # ì§ˆë³‘ ì§„ë‹¨ìš©
        "disease_key": "disease_key_index.index",
        "disease_full": "disease_full_index.index",
        
        # ì˜ì•½í’ˆìš©
        "medication": "medication_index.index"
    }
    
    # ğŸ” ëª¨ë“  ë©”íƒ€ë°ì´í„° íŒŒì¼ ì •ì˜
    ALL_METADATA_FILES = {
        # RAG ë©”íƒ€ë°ì´í„°
        "rag_qa": "rag_qa_documents.pkl",
        "rag_medical": "rag_medical_documents.pkl",
        
        # ì§ˆë³‘ ë©”íƒ€ë°ì´í„°
        "disease": "disease_metadata.pkl",
        
        # ì˜ì•½í’ˆ ë©”íƒ€ë°ì´í„°
        "medication": "medication_metadata.pkl"
    }
    
    CONFIG_FILE = "index_config.json"
    
    def __init__(self):
        """í†µí•© ë¡œë” ì´ˆê¸°í™”"""
        self.indexes = {}
        self.metadata = {}
        self.config = {}
        self.load_status = {}
        
    def load_all_indexes(self) -> Dict[str, Any]:
        """
        ëª¨ë“  FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        
        Returns:
            Dict: ë¡œë“œëœ ì¸ë±ìŠ¤ë“¤ê³¼ ìƒíƒœ ì •ë³´
        """
        logger.info("ğŸš€ í†µí•© FAISS ì¸ë±ìŠ¤ ë¡œë” ì‹œì‘ - ëª¨ë“  10ê°œ íŒŒì¼ í™œìš©")
        
        if not os.path.exists(self.INDEX_DIR):
            logger.error(f"âŒ ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.INDEX_DIR}")
            return self._create_empty_result()
        
        # ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ í™•ì¸
        available_files = os.listdir(self.INDEX_DIR)
        logger.info(f"ğŸ“ ë°œê²¬ëœ íŒŒì¼ ìˆ˜: {len(available_files)}ê°œ")
        
        # ğŸ” 1ë‹¨ê³„: ëª¨ë“  FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        self._load_all_faiss_indexes()
        
        # ğŸ” 2ë‹¨ê³„: ëª¨ë“  ë©”íƒ€ë°ì´í„° ë¡œë“œ
        self._load_all_metadata()
        
        # ğŸ” 3ë‹¨ê³„: ì„¤ì • íŒŒì¼ ë¡œë“œ
        self._load_config()
        
        # ğŸ“Š 4ë‹¨ê³„: ê²°ê³¼ ìš”ì•½
        result = self._summarize_results()
        
        logger.info("âœ… í†µí•© ì¸ë±ìŠ¤ ë¡œë”© ì™„ë£Œ!")
        self._log_summary()
        
        return result
    
    def _load_all_faiss_indexes(self):
        """ëª¨ë“  FAISS ì¸ë±ìŠ¤ íŒŒì¼ ë¡œë“œ"""
        logger.info("ğŸ” FAISS ì¸ë±ìŠ¤ íŒŒì¼ë“¤ ë¡œë“œ ì¤‘...")
        
        for index_name, filename in self.ALL_INDEX_FILES.items():
            file_path = os.path.join(self.INDEX_DIR, filename)
            
            try:
                if os.path.exists(file_path):
                    logger.info(f"ğŸ“‚ {index_name} ì¸ë±ìŠ¤ ë¡œë“œ: {filename}")
                    
                    # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                    index = faiss.read_index(file_path)
                    self.indexes[index_name] = index
                    
                    # ìƒíƒœ ê¸°ë¡
                    self.load_status[index_name] = {
                        "status": "success",
                        "file_size": os.path.getsize(file_path),
                        "vector_count": index.ntotal,
                        "dimension": index.d if hasattr(index, 'd') else 'unknown'
                    }
                    
                    logger.info(f"âœ… {index_name}: {index.ntotal}ê°œ ë²¡í„°, {index.d if hasattr(index, 'd') else 'unknown'}ì°¨ì›")
                    
                else:
                    logger.warning(f"âš ï¸ {index_name} ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {filename}")
                    self.load_status[index_name] = {"status": "file_not_found"}
                    
            except Exception as e:
                logger.error(f"âŒ {index_name} ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.load_status[index_name] = {"status": "load_failed", "error": str(e)}
    
    def _load_all_metadata(self):
        """ëª¨ë“  ë©”íƒ€ë°ì´í„° íŒŒì¼ ë¡œë“œ (CustomUnpickler ì‚¬ìš©)"""
        logger.info("ğŸ” ë©”íƒ€ë°ì´í„° íŒŒì¼ë“¤ ë¡œë“œ ì¤‘...")
        
        for metadata_name, filename in self.ALL_METADATA_FILES.items():
            file_path = os.path.join(self.INDEX_DIR, filename)
            
            try:
                if os.path.exists(file_path):
                    logger.info(f"ğŸ“‚ {metadata_name} ë©”íƒ€ë°ì´í„° ë¡œë“œ: {filename}")
                    
                    # CustomUnpickler ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ë¡œë“œ
                    metadata = self._safe_pickle_load(file_path)
                    self.metadata[metadata_name] = metadata
                    
                    # ìƒíƒœ ê¸°ë¡
                    self.load_status[f"{metadata_name}_metadata"] = {
                        "status": "success",
                        "file_size": os.path.getsize(file_path),
                        "item_count": len(metadata) if isinstance(metadata, list) else "unknown"
                    }
                    
                    logger.info(f"âœ… {metadata_name}: {len(metadata) if isinstance(metadata, list) else 'unknown'}ê°œ í•­ëª©")
                    
                else:
                    logger.warning(f"âš ï¸ {metadata_name} ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {filename}")
                    self.load_status[f"{metadata_name}_metadata"] = {"status": "file_not_found"}
                    
            except Exception as e:
                logger.error(f"âŒ {metadata_name} ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.load_status[f"{metadata_name}_metadata"] = {"status": "load_failed", "error": str(e)}
    
    def _safe_pickle_load(self, file_path: str) -> Any:
        """
        ì•ˆì „í•œ pickle ë¡œë“œ (ë‹¤ì¤‘ fallback ë°©ì‹)
        
        ğŸ”§ í•´ê²° ë°©ë²•:
        1. CustomUnpickler ì‚¬ìš©
        2. ëª¨ë“ˆ ê²½ë¡œ ì„ì‹œ ë§¤í•‘
        3. ê¸€ë¡œë²Œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì£¼ì…
        4. ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ìµœí›„ ìˆ˜ë‹¨)
        """
        
        if not os.path.exists(file_path):
            logger.warning(f"âš ï¸ Pickle íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            return []
        
        file_size = os.path.getsize(file_path)
        logger.debug(f"ğŸ“Š Pickle íŒŒì¼ í¬ê¸°: {file_size} bytes")
        
        if file_size < 10:
            logger.warning(f"âš ï¸ Pickle íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤: {file_size} bytes")
            return []
        
        # ë°©ë²• 1: CustomUnpickler ì‚¬ìš©
        try:
            with open(file_path, 'rb') as f:
                unpickler = CustomUnpickler(f)
                data = unpickler.load()
                
            logger.info(f"âœ… CustomUnpicklerë¡œ ë¡œë“œ ì„±ê³µ: {file_path} ({len(data) if isinstance(data, list) else 'unknown'}ê°œ)")
            return data if isinstance(data, list) else []
            
        except Exception as e1:
            logger.warning(f"âš ï¸ CustomUnpickler ì‹¤íŒ¨: {e1}")
            
            # ë°©ë²• 2: ëª¨ë“ˆ ê²½ë¡œ ì„ì‹œ ë§¤í•‘
            try:
                current_module = sys.modules[__name__]
                temp_mappings = {}
                
                old_paths = [
                    'app.llm.main_llm',
                    'app.services.embedding_service',
                    'app.integration_test.embedding_service',
                    'embedding_service',
                    '__main__'
                ]
                
                # ì„ì‹œ ëª¨ë“ˆ ë§¤í•‘ ì„¤ì •
                for old_path in old_paths:
                    if old_path not in sys.modules:
                        temp_mappings[old_path] = None
                        sys.modules[old_path] = current_module
                
                # pickle ë¡œë“œ ì‹œë„
                with open(file_path, 'rb') as f:
                    data = pickle.load(f)
                
                # ì„ì‹œ ë§¤í•‘ ì •ë¦¬
                for old_path in temp_mappings:
                    if temp_mappings[old_path] is None:
                        sys.modules.pop(old_path, None)
                
                logger.info(f"âœ… ëª¨ë“ˆ ë§¤í•‘ìœ¼ë¡œ ë¡œë“œ ì„±ê³µ: {file_path} ({len(data) if isinstance(data, list) else 'unknown'}ê°œ)")
                return data if isinstance(data, list) else []
                
            except Exception as e2:
                logger.warning(f"âš ï¸ ëª¨ë“ˆ ë§¤í•‘ë„ ì‹¤íŒ¨: {e2}")
                
                # ë°©ë²• 3: ê¸€ë¡œë²Œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì£¼ì…
                try:
                    import builtins
                    
                    # í˜„ì¬ ëª¨ë“ˆì˜ í´ë˜ìŠ¤ë“¤ì„ ê¸€ë¡œë²Œì— ì„ì‹œ ì¶”ê°€
                    current_module = sys.modules[__name__]
                    temp_globals = {}
                    
                    if hasattr(current_module, 'RAGDocument'):
                        builtins.RAGDocument = getattr(current_module, 'RAGDocument')
                        temp_globals['RAGDocument'] = True
                    if hasattr(current_module, 'RAGContentType'):
                        builtins.RAGContentType = getattr(current_module, 'RAGContentType')
                        temp_globals['RAGContentType'] = True
                    
                    # pickle ë¡œë“œ ì‹œë„
                    with open(file_path, 'rb') as f:
                        data = pickle.load(f)
                    
                    # ê¸€ë¡œë²Œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ë¦¬
                    for attr_name in temp_globals:
                        if hasattr(builtins, attr_name):
                            delattr(builtins, attr_name)
                    
                    logger.info(f"âœ… ê¸€ë¡œë²Œ ì£¼ì…ìœ¼ë¡œ ë¡œë“œ ì„±ê³µ: {file_path} ({len(data) if isinstance(data, list) else 'unknown'}ê°œ)")
                    return data if isinstance(data, list) else []
                    
                except Exception as e3:
                    logger.error(f"âŒ ëª¨ë“  pickle ë¡œë“œ ë°©ë²• ì‹¤íŒ¨: {e3}")
                    logger.debug(f"ğŸ” ìµœì¢… ì˜¤ë¥˜ ìƒì„¸: {str(e3)}")
                    
                    # ìµœí›„ ìˆ˜ë‹¨: ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                    logger.warning(f"ğŸ”„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ fallback: {file_path}")
                    return []
    
    def _load_config(self):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        config_path = os.path.join(self.INDEX_DIR, self.CONFIG_FILE)
        
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    self.config = json.load(f)
                logger.info(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {self.CONFIG_FILE}")
                self.load_status["config"] = {"status": "success"}
            except Exception as e:
                logger.error(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.load_status["config"] = {"status": "load_failed", "error": str(e)}
        else:
            logger.info(f"â„¹ï¸ ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.CONFIG_FILE}")
            self.load_status["config"] = {"status": "file_not_found"}
    
    def _summarize_results(self) -> Dict[str, Any]:
        """ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        total_vectors = sum(
            index.ntotal for index in self.indexes.values() 
            if hasattr(index, 'ntotal')
        )
        
        total_metadata_items = sum(
            len(metadata) for metadata in self.metadata.values()
            if isinstance(metadata, list)
        )
        
        successful_indexes = len([
            status for status in self.load_status.values()
            if status.get("status") == "success"
        ])
        
        return {
            "summary": {
                "total_indexes_loaded": len(self.indexes),
                "total_metadata_loaded": len(self.metadata),
                "total_vectors": total_vectors,
                "total_metadata_items": total_metadata_items,
                "successful_loads": successful_indexes,
                "load_timestamp": datetime.now().isoformat()
            },
            "indexes": self.indexes,
            "metadata": self.metadata,
            "config": self.config,
            "load_status": self.load_status
        }
    
    def _log_summary(self):
        """ë¡œë”© ê²°ê³¼ ìš”ì•½ ë¡œê¹…"""
        logger.info("ğŸ“Š === í†µí•© ì¸ë±ìŠ¤ ë¡œë”© ê²°ê³¼ ìš”ì•½ ===")
        
        # ì¸ë±ìŠ¤ë³„ ìƒíƒœ
        for name, index in self.indexes.items():
            if hasattr(index, 'ntotal'):
                logger.info(f"   ğŸ” {name}: {index.ntotal:,}ê°œ ë²¡í„°")
        
        # ë©”íƒ€ë°ì´í„°ë³„ ìƒíƒœ
        for name, metadata in self.metadata.items():
            if isinstance(metadata, list):
                logger.info(f"   ğŸ“‹ {name}: {len(metadata):,}ê°œ í•­ëª©")
        
        # ì „ì²´ í†µê³„
        total_vectors = sum(index.ntotal for index in self.indexes.values() if hasattr(index, 'ntotal'))
        total_items = sum(len(m) for m in self.metadata.values() if isinstance(m, list))
        
        logger.info(f"   ğŸ“Š ì´ ë²¡í„°: {total_vectors:,}ê°œ")
        logger.info(f"   ğŸ“Š ì´ ë©”íƒ€ë°ì´í„°: {total_items:,}ê°œ")
        logger.info(f"   ğŸ“Š ë¡œë“œëœ ì¸ë±ìŠ¤: {len(self.indexes)}ê°œ")
        logger.info("==========================================")
    
    def _create_empty_result(self) -> Dict[str, Any]:
        """ë¹ˆ ê²°ê³¼ ìƒì„±"""
        return {
            "summary": {
                "total_indexes_loaded": 0,
                "total_metadata_loaded": 0,
                "total_vectors": 0,
                "total_metadata_items": 0,
                "successful_loads": 0,
                "load_timestamp": datetime.now().isoformat()
            },
            "indexes": {},
            "metadata": {},
            "config": {},
            "load_status": {"error": "index_directory_not_found"}
        }

# =============================================================================
# ğŸ”„ ê°œì„ ëœ RAG ì¸ë±ìŠ¤ ë§¤ë‹ˆì € - í†µí•© ë¡œë” ì‚¬ìš©
# =============================================================================

class EnhancedRAGIndexManager:
    """
    ê°œì„ ëœ RAG ì¸ë±ìŠ¤ ë§¤ë‹ˆì € - ëª¨ë“  ì¸ë±ìŠ¤ í™œìš©
    
    ğŸš€ ê¸°ëŠ¥:
    - í†µí•© ì¸ë±ìŠ¤ ë¡œë” ì‚¬ìš©
    - ëª¨ë“  FAISS ì¸ë±ìŠ¤ í™œìš© (RAG + ì§ˆë³‘ + ì˜ì•½í’ˆ)
    - í–¥ìƒëœ ê²€ìƒ‰ ì„±ëŠ¥
    - ë©€í‹° ì¸ë±ìŠ¤ í†µí•© ê²€ìƒ‰
    
    ğŸ“‹ ì§€ì› ê²€ìƒ‰:
    - RAG Q&A ê²€ìƒ‰
    - ì˜ë£Œ ë¬¸ì„œ ê²€ìƒ‰
    - ì§ˆë³‘ ì •ë³´ ê²€ìƒ‰
    - ì˜ì•½í’ˆ ì •ë³´ ê²€ìƒ‰
    - í†µí•© ê²€ìƒ‰ (ëª¨ë“  ì¸ë±ìŠ¤ ë™ì‹œ ê²€ìƒ‰)
    """
    
    def __init__(self, embedding_model: EmbeddingModel):
        """ê°œì„ ëœ RAG ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        self.embedding_model = embedding_model
        
        # í†µí•© ë¡œë” ì‚¬ìš©
        self.unified_loader = UnifiedIndexLoader()
        
        # ì¸ë±ìŠ¤ë“¤
        self.qa_index = None
        self.medical_doc_index = None
        self.disease_key_index = None
        self.disease_full_index = None
        self.medication_index = None
        
        # ë©”íƒ€ë°ì´í„°ë“¤
        self.qa_documents = []
        self.medical_documents = []
        self.disease_metadata = []
        self.medication_metadata = []
        
        # ì„¤ì •
        self.config = {}
        
        logger.info("ğŸ” ê°œì„ ëœ RAG ë§¤ë‹ˆì € ì´ˆê¸°í™”: ëª¨ë“  ì¸ë±ìŠ¤ í™œìš©")
    
    def load_rag_data(self):
        """ëª¨ë“  RAG ë°ì´í„° ë¡œë“œ (í†µí•© ë°©ì‹)"""
        logger.info("ğŸ”„ í†µí•© RAG ë°ì´í„° ë¡œë”© ì‹œì‘...")
        
        # ğŸš€ í†µí•© ë¡œë”ë¡œ ëª¨ë“  ì¸ë±ìŠ¤ ë¡œë“œ
        result = self.unified_loader.load_all_indexes()
        
        # ğŸ“‹ ì¸ë±ìŠ¤ í• ë‹¹
        self._assign_indexes(result["indexes"])
        
        # ğŸ“‹ ë©”íƒ€ë°ì´í„° í• ë‹¹
        self._assign_metadata(result["metadata"])
        
        # ğŸ“‹ ì„¤ì • í• ë‹¹
        self.config = result["config"]
        
        # ğŸ“Š ê²°ê³¼ ë¡œê¹…
        self._log_final_status()
        
        logger.info("âœ… í†µí•© RAG ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
    
    def _assign_indexes(self, indexes: Dict[str, Any]):
        """ì¸ë±ìŠ¤ í• ë‹¹"""
        self.qa_index = indexes.get("rag_qa")
        self.medical_doc_index = indexes.get("rag_medical")
        self.disease_key_index = indexes.get("disease_key")
        self.disease_full_index = indexes.get("disease_full")
        self.medication_index = indexes.get("medication")
        
        logger.info("âœ… ëª¨ë“  ì¸ë±ìŠ¤ í• ë‹¹ ì™„ë£Œ")
    
    def _assign_metadata(self, metadata: Dict[str, Any]):
        """ë©”íƒ€ë°ì´í„° í• ë‹¹"""
        self.qa_documents = metadata.get("rag_qa", [])
        self.medical_documents = metadata.get("rag_medical", [])
        self.disease_metadata = metadata.get("disease", [])
        self.medication_metadata = metadata.get("medication", [])
        
        logger.info("âœ… ëª¨ë“  ë©”íƒ€ë°ì´í„° í• ë‹¹ ì™„ë£Œ")
    
    def _log_final_status(self):
        """ìµœì¢… ìƒíƒœ ë¡œê¹…"""
        logger.info("ğŸ“Š === ìµœì¢… ì¸ë±ìŠ¤ ìƒíƒœ ===")
        
        # ê° ì¸ë±ìŠ¤ ìƒíƒœ
        indexes_info = [
            ("RAG Q&A", self.qa_index, len(self.qa_documents)),
            ("RAG Medical", self.medical_doc_index, len(self.medical_documents)),
            ("Disease Key", self.disease_key_index, len(self.disease_metadata)),
            ("Disease Full", self.disease_full_index, 0),
            ("Medication", self.medication_index, len(self.medication_metadata))
        ]
        
        for name, index, metadata_count in indexes_info:
            if index:
                logger.info(f"   âœ… {name}: {index.ntotal:,}ê°œ ë²¡í„°, {metadata_count:,}ê°œ ë©”íƒ€ë°ì´í„°")
            else:
                logger.info(f"   âŒ {name}: ë¡œë“œ ì‹¤íŒ¨")
        
        # ì „ì²´ í†µê³„
        total_vectors = sum(
            index.ntotal for index in [
                self.qa_index, self.medical_doc_index, 
                self.disease_key_index, self.disease_full_index, 
                self.medication_index
            ] if index is not None
        )
        
        total_metadata = len(self.qa_documents) + len(self.medical_documents) + \
                        len(self.disease_metadata) + len(self.medication_metadata)
        
        logger.info(f"   ğŸ“Š ì „ì²´ ë²¡í„°: {total_vectors:,}ê°œ")
        logger.info(f"   ğŸ“Š ì „ì²´ ë©”íƒ€ë°ì´í„°: {total_metadata:,}ê°œ")
        logger.info("===========================")
    
    # =============================================================================
    # ğŸ” ê¸°ë³¸ ê²€ìƒ‰ ë©”ì„œë“œë“¤
    # =============================================================================
    
    def search_qa(self, query: str, top_k: int = 5) -> List[Tuple[Any, float]]:
        """RAG Q&A ê²€ìƒ‰"""
        if not self.qa_index or not self.qa_documents:
            logger.warning("âš ï¸ RAG Q&A ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        try:
            query_embedding = self.embedding_model.encode_single(query)
            scores, indices = self.qa_index.search(
                query_embedding.reshape(1, -1).astype('float32'), top_k
            )
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if 0 <= idx < len(self.qa_documents):
                    results.append((self.qa_documents[idx], float(score)))
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ RAG Q&A ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_medical_docs(self, query: str, top_k: int = 5) -> List[Tuple[Any, float]]:
        """RAG ì˜ë£Œ ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.medical_doc_index or not self.medical_documents:
            logger.warning("âš ï¸ RAG Medical ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        try:
            query_embedding = self.embedding_model.encode_single(query)
            scores, indices = self.medical_doc_index.search(
                query_embedding.reshape(1, -1).astype('float32'), top_k
            )
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if 0 <= idx < len(self.medical_documents):
                    results.append((self.medical_documents[idx], float(score)))
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ RAG Medical ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_disease_advanced(self, query: str, top_k: int = 5) -> List[Tuple[Any, float]]:
        """ê³ ê¸‰ ì§ˆë³‘ ê²€ìƒ‰ (disease ì¸ë±ìŠ¤ í™œìš©)"""
        if not self.disease_key_index:
            logger.warning("âš ï¸ Disease ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        try:
            query_embedding = self.embedding_model.encode_single(query)
            scores, indices = self.disease_key_index.search(
                query_embedding.reshape(1, -1).astype('float32'), top_k
            )
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if 0 <= idx < len(self.disease_metadata):
                    results.append((self.disease_metadata[idx], float(score)))
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ê³ ê¸‰ ì§ˆë³‘ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_medication_advanced(self, query: str, top_k: int = 5) -> List[Tuple[Any, float]]:
        """ê³ ê¸‰ ì˜ì•½í’ˆ ê²€ìƒ‰ (medication ì¸ë±ìŠ¤ í™œìš©)"""
        if not self.medication_index:
            logger.warning("âš ï¸ Medication ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        try:
            query_embedding = self.embedding_model.encode_single(query)
            scores, indices = self.medication_index.search(
                query_embedding.reshape(1, -1).astype('float32'), top_k
            )
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if 0 <= idx < len(self.medication_metadata):
                    results.append((self.medication_metadata[idx], float(score)))
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ê³ ê¸‰ ì˜ì•½í’ˆ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    # =============================================================================
    # ğŸ”„ í†µí•© ê²€ìƒ‰ ë©”ì„œë“œë“¤
    # =============================================================================
    
    def search_unified(self, query: str, top_k: int = 3) -> Dict[str, List[Tuple[Any, float]]]:
        """í†µí•© ê²€ìƒ‰ - ëª¨ë“  ì¸ë±ìŠ¤ í™œìš©"""
        return {
            "qa_results": self.search_qa(query, top_k),
            "medical_results": self.search_medical_docs(query, top_k),
            "disease_results": self.search_disease_advanced(query, top_k),
            "medication_results": self.search_medication_advanced(query, top_k)
        }
    
    def search_combined(self, query: str, qa_top_k: int = 3, medical_top_k: int = 3) -> Dict[str, List[Tuple[Any, float]]]:
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ í†µí•© ê²€ìƒ‰ (Q&A + ì˜ë£Œ ë¬¸ì„œ)"""
        return {
            "qa_results": self.search_qa(query, qa_top_k),
            "medical_results": self.search_medical_docs(query, medical_top_k)
        }
    
    def search_best_match(self, query: str, search_types: List[str] = None) -> Tuple[Any, float, str]:
        """
        ìµœê³  ì ìˆ˜ ê²°ê³¼ ë°˜í™˜ (ëª¨ë“  ì¸ë±ìŠ¤ì—ì„œ)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            search_types: ê²€ìƒ‰í•  ì¸ë±ìŠ¤ íƒ€ì… ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  íƒ€ì…)
            
        Returns:
            Tuple[ê²°ê³¼, ì ìˆ˜, ì¸ë±ìŠ¤_íƒ€ì…]
        """
        if search_types is None:
            search_types = ["qa", "medical", "disease", "medication"]
        
        all_results = []
        
        # ê° ì¸ë±ìŠ¤ì—ì„œ ìµœê³  ì ìˆ˜ ê²°ê³¼ ìˆ˜ì§‘
        if "qa" in search_types:
            qa_results = self.search_qa(query, 1)
            if qa_results:
                all_results.append((qa_results[0][0], qa_results[0][1], "qa"))
        
        if "medical" in search_types:
            medical_results = self.search_medical_docs(query, 1)
            if medical_results:
                all_results.append((medical_results[0][0], medical_results[0][1], "medical"))
        
        if "disease" in search_types:
            disease_results = self.search_disease_advanced(query, 1)
            if disease_results:
                all_results.append((disease_results[0][0], disease_results[0][1], "disease"))
        
        if "medication" in search_types:
            medication_results = self.search_medication_advanced(query, 1)
            if medication_results:
                all_results.append((medication_results[0][0], medication_results[0][1], "medication"))
        
        if not all_results:
            return None, 0.0, "none"
        
        # ìµœê³  ì ìˆ˜ ê²°ê³¼ ë°˜í™˜
        best_result = max(all_results, key=lambda x: x[1])
        return best_result
    
    # =============================================================================
    # ğŸ”§ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
    # =============================================================================
    
    def get_index_status(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        return {
            "indexes_loaded": {
                "qa": self.qa_index is not None,
                "medical": self.medical_doc_index is not None,
                "disease_key": self.disease_key_index is not None,
                "disease_full": self.disease_full_index is not None,
                "medication": self.medication_index is not None
            },
            "vector_counts": {
                "qa": self.qa_index.ntotal if self.qa_index else 0,
                "medical": self.medical_doc_index.ntotal if self.medical_doc_index else 0,
                "disease_key": self.disease_key_index.ntotal if self.disease_key_index else 0,
                "disease_full": self.disease_full_index.ntotal if self.disease_full_index else 0,
                "medication": self.medication_index.ntotal if self.medication_index else 0
            },
            "metadata_counts": {
                "qa": len(self.qa_documents),
                "medical": len(self.medical_documents),
                "disease": len(self.disease_metadata),
                "medication": len(self.medication_metadata)
            },
            "config": self.config
        }

# =============================================================================
# ğŸ”„ ê¸°ì¡´ ì½”ë“œì™€ì˜ í˜¸í™˜ì„± ìœ ì§€
# =============================================================================

# ê¸°ì¡´ RAGIndexManagerë¥¼ EnhancedRAGIndexManagerë¡œ ëŒ€ì²´í•˜ë˜, ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€
RAGIndexManager = EnhancedRAGIndexManager

# =============================================================================
# ğŸš€ ë©”ì¸ ì´ˆê¸°í™” í•¨ìˆ˜
# =============================================================================

def initialize_embedding_service() -> Tuple[EmbeddingModel, EnhancedRAGIndexManager]:
    """
    ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    
    Returns:
        Tuple[EmbeddingModel, EnhancedRAGIndexManager]: ì„ë² ë”© ëª¨ë¸ê³¼ RAG ë§¤ë‹ˆì €
    """
    logger.info("ğŸš€ ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹œì‘...")
    
    try:
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embedding_model = EmbeddingModel()
        
        # RAG ë§¤ë‹ˆì € ì´ˆê¸°í™”
        rag_manager = EnhancedRAGIndexManager(embedding_model)
        
        # RAG ë°ì´í„° ë¡œë“œ
        rag_manager.load_rag_data()
        
        logger.info("âœ… ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!")
        
        return embedding_model, rag_manager
        
    except Exception as e:
        logger.error(f"âŒ ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise e

# =============================================================================
# ğŸ”§ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì½”ë“œ
# =============================================================================

if __name__ == "__main__":
    """ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ§ª ì„ë² ë”© ì„œë¹„ìŠ¤ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        embedding_model, rag_manager = initialize_embedding_service()
        
        # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        test_query = "ë‘í†µì´ ìˆì–´ìš”"
        
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{test_query}'")
        
        # í†µí•© ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        unified_results = rag_manager.search_unified(test_query, top_k=2)
        
        for search_type, results in unified_results.items():
            print(f"\nğŸ“‹ {search_type}: {len(results)}ê°œ ê²°ê³¼")
            for i, (doc, score) in enumerate(results[:2]):
                print(f"   {i+1}. ì ìˆ˜: {score:.4f}")
                if hasattr(doc, 'content'):
                    print(f"      ë‚´ìš©: {doc.content[:100]}...")
                else:
                    print(f"      ë‚´ìš©: {str(doc)[:100]}...")
        
        # ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸
        status = rag_manager.get_index_status()
        print(f"\nğŸ“Š ì¸ë±ìŠ¤ ìƒíƒœ:")
        print(f"   ë¡œë“œëœ ì¸ë±ìŠ¤: {sum(status['indexes_loaded'].values())}ê°œ")
        print(f"   ì´ ë²¡í„° ìˆ˜: {sum(status['vector_counts'].values()):,}ê°œ")
        print(f"   ì´ ë©”íƒ€ë°ì´í„°: {sum(status['metadata_counts'].values()):,}ê°œ")
        
        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()